# crawler_otimizado_compativel.py - 100% compat√≠vel com auditoria SEO original

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
import os
import pickle
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

warnings.filterwarnings("ignore")

# ========================
# üöÄ Session Manager Otimizado
# ========================

class FastSession:
    def __init__(self):
        self.session = requests.Session()
        
        # Headers otimizados
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive"
        })
        
        # Configura√ß√µes para velocidade MAS sem afetar auditoria
        retry_strategy = Retry(
            total=3,  # Mais retries para n√£o perder URLs
            status_forcelist=[429, 500, 502, 503, 504],
            backoff_factor=0.2  # Backoff um pouco mais lento para ser mais conservador
        )
        
        adapter = HTTPAdapter(
            pool_connections=20,  # Reduzido para ser mais conservador
            pool_maxsize=30,      # Reduzido para evitar problemas
            max_retries=retry_strategy
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def get(self, url, **kwargs):
        # TIMEOUT ORIGINAL MANTIDO para n√£o perder URLs lentas
        kwargs.setdefault('timeout', 10)  # Mant√©m timeout original de 10s
        kwargs.setdefault('verify', False)
        return self.session.get(url, **kwargs)

# Inst√¢ncia global da sess√£o
fast_session = FastSession()

# ========================
# üö¶ Fun√ß√µes Auxiliares - L√ìGICA ORIGINAL PRESERVADA
# ========================

def link_eh_util(href):
    """L√ìGICA ORIGINAL EXATA - n√£o modificada"""
    if not href or href.startswith(('mailto:', 'tel:', 'javascript:', '#')):
        return False
    if any(href.lower().endswith(ext) for ext in ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.zip']):
        return False
    return True

def normalizar_url_conservadora(url, base_url):
    """Normaliza√ß√£o CONSERVADORA - mant√©m URLs originais tanto quanto poss√≠vel"""
    try:
        if not url or not url.strip():
            return None
        
        url = url.strip()
        
        # PRESERVA L√ìGICA ORIGINAL: apenas urljoin + split('#')[0] + rstrip('/')
        full_url = urljoin(base_url, url.split('#')[0])
        parsed = urlparse(full_url)
        
        if parsed.scheme not in ['http', 'https']:
            return None
        
        # MANT√âM URL ORIGINAL - s√≥ remove fragmento e trailing slash
        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if parsed.query:
            clean_url += f"?{parsed.query}"  # PRESERVA TODOS os par√¢metros para auditoria
        
        return clean_url.rstrip('/')
    except:
        return None

def processar_url_compativel(url, nivel, dominio_base):
    """Processamento compat√≠vel com estrutura original"""
    try:
        start_time = time.time()
        response = fast_session.get(url)
        response_time = (time.time() - start_time) * 1000
        
        # ESTRUTURA ORIGINAL PRESERVADA + dados extras opcionais
        resultado = {
            # CAMPOS ORIGINAIS OBRIGAT√ìRIOS (mesma ordem)
            "url": url,
            "nivel": nivel, 
            "status_code": response.status_code,  # NOME ORIGINAL
            "tipo_conteudo": response.headers.get("Content-Type", "desconhecido"),
            
            # CAMPOS EXTRAS (n√£o afetam compatibilidade)
            "response_time": round(response_time, 2),
            "redirected": response.url != url,
            "final_url": response.url
        }
        
        # Extrai links APENAS se for HTML com sucesso (como original)
        if (response.status_code == 200 and 
            'text/html' in resultado["tipo_conteudo"]):
            
            # USA LXML como original
            soup = BeautifulSoup(response.text, 'lxml')
            
            # L√ìGICA ORIGINAL DE EXTRA√á√ÉO DE LINKS
            links_encontrados = []
            for tag in soup.find_all("a", href=True):
                href = tag["href"]
                if link_eh_util(href):  # USA FUN√á√ÉO ORIGINAL
                    # L√ìGICA ORIGINAL: urljoin + split('#')[0] + rstrip('/')
                    href_normalizada = urljoin(url, href.split("#")[0].rstrip("/"))
                    if urlparse(href_normalizada).netloc == dominio_base:
                        links_encontrados.append(href_normalizada)
            
            # Adiciona links como campo extra (n√£o afeta compatibilidade)
            resultado["links_encontrados"] = list(set(links_encontrados))
        
        return resultado
        
    except Exception as e:
        # ESTRUTURA DE ERRO ORIGINAL
        return {
            "url": url,
            "nivel": nivel,
            "status_code": None,
            "tipo_conteudo": f"Erro: {str(e)}"
        }

def salvar_cache(nome_arquivo, dados):
    """FUN√á√ÉO ORIGINAL"""
    with open(nome_arquivo, 'wb') as f:
        pickle.dump(dados, f)

def carregar_cache(nome_arquivo):
    """FUN√á√ÉO ORIGINAL"""
    if os.path.exists(nome_arquivo):
        with open(nome_arquivo, 'rb') as f:
            return pickle.load(f)
    return None

def excluir_cache(nome_arquivo):
    """FUN√á√ÉO ORIGINAL"""
    if os.path.exists(nome_arquivo):
        os.remove(nome_arquivo)
        print(f"üßπ Cache antigo exclu√≠do: {nome_arquivo}")

# ========================
# üîç Fun√ß√£o Principal - ASSINATURA ORIGINAL + Threading
# ========================

def rastrear_profundo(url_inicial, max_urls=1000, max_depth=3, forcar_reindexacao=False, max_workers=10):
    """
    FUN√á√ÉO ORIGINAL com threading opcional
    
    ASSINATURA COMPAT√çVEL: todos os par√¢metros originais mantidos
    NOVO PAR√ÇMETRO: max_workers (padr√£o conservador de 10)
    """
    
    # L√ìGICA DE CACHE ORIGINAL
    cache_path = f".cache_{urlparse(url_inicial).netloc.replace('.', '_')}.pkl"

    if forcar_reindexacao:
        excluir_cache(cache_path)

    if not forcar_reindexacao:
        cache = carregar_cache(cache_path)
        if cache:
            print(f"‚ôªÔ∏è Cache encontrado: {cache_path}")
            return cache

    # Log compat√≠vel
    print(f"üöÄ Crawler otimizado para: {url_inicial}")
    print(f"üìä Config: {max_urls} URLs m√°x, profundidade {max_depth}, {max_workers} workers")

    # ESTRUTURAS ORIGINAIS
    visitadas = set()
    fila = [(url_inicial, 0)]  # ESTRUTURA ORIGINAL: lista de tuplas
    resultados = []
    dominio_base = urlparse(url_inicial).netloc

    # PROGRESS BAR ORIGINAL
    with tqdm(total=max_urls, desc="üîç Rastreamento Otimizado") as pbar:
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            
            while fila and len(resultados) < max_urls:
                # Cria lote de URLs para processar
                lote_urls = []
                futures = []
                
                # L√ìGICA ORIGINAL: pega URLs da fila
                while fila and len(lote_urls) < max_workers and len(resultados) + len(lote_urls) < max_urls:
                    url_atual, nivel = fila.pop(0)  # MANT√âM pop(0) original
                    
                    # CONDI√á√ïES ORIGINAIS
                    if url_atual not in visitadas and nivel <= max_depth:
                        lote_urls.append((url_atual, nivel))
                        visitadas.add(url_atual)
                        
                        # Submete tarefa com threading
                        future = executor.submit(processar_url_compativel, url_atual, nivel, dominio_base)
                        futures.append(future)
                
                # Processa resultados
                for future in as_completed(futures):
                    try:
                        resultado = future.result(timeout=30)
                        resultados.append(resultado)
                        
                        # L√ìGICA ORIGINAL de adi√ß√£o de links √† fila
                        if resultado.get("links_encontrados") and resultado["nivel"] < max_depth:
                            for link in resultado["links_encontrados"]:
                                if link not in visitadas and len(visitadas) + len(fila) < max_urls:
                                    fila.append((link, resultado["nivel"] + 1))  # ESTRUTURA ORIGINAL
                        
                        pbar.update(1)
                        
                    except Exception as e:
                        print(f"‚ùå Erro processando: {e}")
                        pbar.update(1)

    # CACHE ORIGINAL
    salvar_cache(cache_path, resultados)
    
    # Log final compat√≠vel
    print(f"‚úÖ Crawl conclu√≠do: {len(resultados)} URLs processadas")
    
    return resultados

# ========================
# üß™ Modo de Compatibilidade Total
# ========================

def rastrear_profundo_original(url_inicial, max_urls=1000, max_depth=3, forcar_reindexacao=False):
    """
    Modo 100% original (sem threading) para m√°xima compatibilidade
    Use esta fun√ß√£o se tiver problemas com a vers√£o threading
    """
    return rastrear_profundo(url_inicial, max_urls, max_depth, forcar_reindexacao, max_workers=1)

# ========================
# üîß Fun√ß√£o de Teste de Compatibilidade
# ========================

def testar_compatibilidade():
    """Testa se a estrutura de retorno est√° correta"""
    print("üß™ Testando compatibilidade...")
    
    # Teste com URL fict√≠cia
    resultado_teste = {
        "url": "https://example.com",
        "nivel": 0,
        "status_code": 200,
        "tipo_conteudo": "text/html"
    }
    
    campos_obrigatorios = ["url", "nivel", "status_code", "tipo_conteudo"]
    
    for campo in campos_obrigatorios:
        if campo in resultado_teste:
            print(f"‚úÖ Campo '{campo}' presente")
        else:
            print(f"‚ùå Campo '{campo}' AUSENTE - PROBLEMA!")
    
    print("‚úÖ Teste de compatibilidade conclu√≠do")

if __name__ == "__main__":
    # Teste r√°pido
    testar_compatibilidade()
    
    print("\nüöÄ Exemplo de uso:")
    print("# Vers√£o otimizada com threading:")
    print("resultados = rastrear_profundo('https://example.com', max_workers=15)")
    print("\n# Vers√£o 100% original:")
    print("resultados = rastrear_profundo_original('https://example.com')")