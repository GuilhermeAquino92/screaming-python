# crawler_playwright.py - Enterprise SEO Crawler com URLManager SEO Integrado

import asyncio
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
from playwright._impl._errors import TimeoutError as PlaywrightTimeoutError
from urllib.parse import urljoin, urlparse
from tqdm.asyncio import tqdm
import time
import os
import pickle
import warnings
from concurrent.futures import ThreadPoolExecutor
import json
from typing import List, Dict, Optional, Tuple
import logging

# ğŸ†• IMPORT DO URL MANAGER SEO
from url_manager_seo import URLManagerSEO

warnings.filterwarnings("ignore")

# ========================
# ğŸ¯ ConfiguraÃ§Ãµes Enterprise (MANTÃ‰M SEU CÃ“DIGO ORIGINAL)
# ========================

class PlaywrightConfig:
    """ğŸ”§ ConfiguraÃ§Ãµes otimizadas para SEO enterprise"""
    
    # ğŸš€ Performance Settings
    BROWSER_POOL_SIZE = 3  # NÃºmero de browsers paralelos
    MAX_CONCURRENT_PAGES = 10  # PÃ¡ginas simultÃ¢neas por browser
    PAGE_TIMEOUT = 30000  # 30s timeout (sites complexos)
    NAVIGATION_TIMEOUT = 20000  # 20s para navegaÃ§Ã£o
    
    # ğŸ¯ SEO Settings
    USER_AGENT = "Mozilla/5.0 (compatible; SEO-Analyzer/1.0; +https://seo-analyzer.com/bot)"
    VIEWPORT = {"width": 1366, "height": 768}  # Desktop padrÃ£o para SEO
    
    # ğŸ“Š Content Settings
    WAIT_FOR_NETWORK_IDLE = True  # Aguarda AJAX/lazy loading
    WAIT_FOR_FONTS = True  # Aguarda fontes (afeta rendering)
    COLLECT_CONSOLE_ERRORS = True  # Debug JS errors
    
    # ğŸ” SEO Detection
    JS_FRAMEWORKS_PATTERNS = [
        'react', 'vue', 'angular', 'next.js', 'nuxt', 'gatsby',
        'spa', 'ajax', 'dynamic', 'client-side'
    ]

# ========================
# ğŸ§  DetecÃ§Ã£o Inteligente de Sites (MANTÃ‰M SEU CÃ“DIGO)
# ========================

class SiteAnalyzer:
    """ğŸ” Detecta se site precisa de JavaScript para SEO"""
    
    @staticmethod
    async def needs_javascript(page: Page, url: str) -> Tuple[bool, str]:
        """
        ğŸ¯ Detecta se site precisa de JS para renderizaÃ§Ã£o completa
        
        Returns:
            (needs_js: bool, reason: str)
        """
        try:
            reasons = []
            
            # 1. ğŸ” Detecta frameworks JS no HTML
            html_content = await page.content()
            
            # Patterns de frameworks
            js_patterns = [
                ('react', 'data-reactroot'),
                ('vue', 'v-'),
                ('angular', 'ng-'),
                ('next.js', '__NEXT_DATA__'),
                ('nuxt', '__NUXT__'),
                ('gatsby', '___gatsby'),
            ]
            
            for framework, pattern in js_patterns:
                if pattern in html_content.lower():
                    reasons.append(f"Framework {framework} detectado")
            
            # 2. ğŸ¯ Verifica se title/meta sÃ£o dinÃ¢micos
            initial_title = await page.title()
            
            # Aguarda JS executar
            await page.wait_for_timeout(2000)
            await page.wait_for_load_state('networkidle', timeout=10000)
            
            final_title = await page.title()
            
            if initial_title != final_title:
                reasons.append(f"Title dinÃ¢mico: '{initial_title}' â†’ '{final_title}'")
            
            # 3. ğŸ” Detecta lazy loading / AJAX
            script_tags = await page.query_selector_all('script')
            has_ajax = False
            
            for script in script_tags:
                script_content = await script.get_attribute('src') or ''
                if any(term in script_content.lower() for term in ['ajax', 'fetch', 'axios', 'xhr']):
                    has_ajax = True
                    break
            
            if has_ajax:
                reasons.append("AJAX/Fetch detectado")
            
            # 4. ğŸ¯ Verifica se hÃ¡ mais headings apÃ³s JS
            initial_headings = await page.query_selector_all('h1, h2, h3, h4, h5, h6')
            
            # Simula scroll para trigger lazy loading
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(1000)
            
            final_headings = await page.query_selector_all('h1, h2, h3, h4, h5, h6')
            
            if len(final_headings) > len(initial_headings):
                reasons.append(f"Headings dinÃ¢micos: {len(initial_headings)} â†’ {len(final_headings)}")
            
            # ğŸ¯ DecisÃ£o final
            needs_js = len(reasons) > 0
            reason_text = " | ".join(reasons) if reasons else "Site estÃ¡tico detectado"
            
            return needs_js, reason_text
            
        except Exception as e:
            return True, f"Erro na detecÃ§Ã£o, usando JS por seguranÃ§a: {str(e)}"

# ========================
# ğŸš€ Browser Pool Manager (MANTÃ‰M SEU CÃ“DIGO ORIGINAL)
# ========================

class BrowserPool:
    """ğŸ¯ Gerencia pool de browsers para performance mÃ¡xima"""
    
    def __init__(self, pool_size: int = 3):
        self.pool_size = pool_size
        self.browsers: List[Browser] = []
        self.contexts: List[BrowserContext] = []
        self.semaphore = asyncio.Semaphore(pool_size * PlaywrightConfig.MAX_CONCURRENT_PAGES)
        
    async def initialize(self, playwright):
        """ğŸš€ Inicializa pool de browsers"""
        print(f"ğŸš€ Inicializando pool de {self.pool_size} browsers...")
        
        for i in range(self.pool_size):
            browser = await playwright.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-images',  # ğŸš€ OtimizaÃ§Ã£o: nÃ£o carrega imagens
                    '--disable-javascript-harmony-shipping',
                    '--disable-background-timer-throttling',
                    '--disable-renderer-backgrounding',
                    '--disable-backgrounding-occluded-windows',
                ]
            )
            
            context = await browser.new_context(
                user_agent=PlaywrightConfig.USER_AGENT,
                viewport=PlaywrightConfig.VIEWPORT,
                ignore_https_errors=True,
                java_script_enabled=True,
                accept_downloads=False,
            )
            
            self.browsers.append(browser)
            self.contexts.append(context)
        
        print(f"âœ… Pool de browsers inicializado com sucesso!")
    
    async def get_page(self) -> Tuple[Page, int]:
        """ğŸ¯ ObtÃ©m pÃ¡gina do pool com load balancing"""
        await self.semaphore.acquire()
        
        # Round-robin simples
        context_index = len([c for c in self.contexts if len(c.pages) > 0]) % len(self.contexts)
        context = self.contexts[context_index]
        
        page = await context.new_page()
        
        # ğŸ¯ ConfiguraÃ§Ãµes SEO especÃ­ficas
        await page.set_extra_http_headers({
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        })
        
        return page, context_index
    
    async def release_page(self, page: Page):
        """ğŸ“¤ Libera pÃ¡gina de volta ao pool"""
        try:
            await page.close()
        except:
            pass
        finally:
            self.semaphore.release()
    
    async def close_all(self):
        """ğŸ”š Fecha todos os browsers do pool"""
        print("ğŸ”š Fechando pool de browsers...")
        for browser in self.browsers:
            try:
                await browser.close()
            except:
                pass

# ========================
# ğŸ¯ Processador de URL Individual (SEU CÃ“DIGO + TITLE V5)
# ========================

async def processar_url_playwright(
    url: str, 
    nivel: int, 
    dominio_base: str, 
    browser_pool: BrowserPool,
    config: PlaywrightConfig = PlaywrightConfig()
) -> Dict:
    """ğŸ¯ Processa uma URL com Playwright (enterprise-grade)"""
    
    page = None
    start_time = time.time()
    
    try:
        # ğŸš€ ObtÃ©m pÃ¡gina do pool
        page, browser_index = await browser_pool.get_page()
        
        # ğŸ“Š ConfiguraÃ§Ãµes de timeout
        page.set_default_timeout(config.PAGE_TIMEOUT)
        page.set_default_navigation_timeout(config.NAVIGATION_TIMEOUT)
        
        # ğŸ¯ Coleta erros de console (debugging)
        console_errors = []
        if config.COLLECT_CONSOLE_ERRORS:
            page.on("console", lambda msg: 
                console_errors.append(msg.text) if msg.type == "error" else None
            )
        
        # ğŸš€ NavegaÃ§Ã£o com otimizaÃ§Ãµes
        try:
            response = await page.goto(
                url,
                wait_until='domcontentloaded',  # Mais rÃ¡pido que 'load'
                timeout=config.NAVIGATION_TIMEOUT
            )
        except PlaywrightTimeoutError as e:
            print(f"âš ï¸ Timeout na navegaÃ§Ã£o: {url}")
            response = None
        
        # â±ï¸ Aguarda estabilizaÃ§Ã£o (crÃ­tico para SEO)
        if config.WAIT_FOR_NETWORK_IDLE:
            try:
                await page.wait_for_load_state('networkidle', timeout=10000)
            except:
                pass  # NÃ£o bloqueia se timeout
        
        # ğŸ¯ Aguarda fontes carregarem (afeta rendering)
        if config.WAIT_FOR_FONTS:
            try:
                await page.wait_for_function(
                    "document.fonts.status === 'loaded'",
                    timeout=5000
                )
            except:
                pass
        
        # ğŸ“Š Coleta dados SEO completos
        resultado = await extrair_dados_seo_completos(page, url, nivel)
        
        # ğŸ” Detecta se site precisa de JS
        needs_js, js_reason = await SiteAnalyzer.needs_javascript(page, url)
        
        # âš¡ Performance metrics
        response_time = (time.time() - start_time) * 1000
        
        # ğŸ¯ Dados de retorno (compatÃ­vel com estrutura original)
        resultado.update({
            # ğŸ“Š Dados originais mantidos
            "url": url,
            "nivel": nivel,
            "status_code_http": response.status if response else None,
            "tipo_conteudo": response.headers.get("content-type", "unknown") if response else "unknown",
            
            # ğŸ†• Dados enterprise
            "response_time": round(response_time, 2),
            "browser_index": browser_index,
            "needs_javascript": needs_js,
            "js_detection_reason": js_reason,
            "console_errors": console_errors[:5],  # MÃ¡ximo 5 erros
            "final_url": page.url,  # URL apÃ³s redirects
            "redirected": page.url != url,
            
            # ğŸ” Links encontrados
            "links_encontrados": await extrair_links_internos(page, dominio_base)
        })
        
        return resultado
        
    except Exception as e:
        # ğŸ“Š Estrutura de erro compatÃ­vel
        return {
            "url": url,
            "nivel": nivel,
            "status_code_http": None,
            "tipo_conteudo": f"Erro Playwright: {str(e)}",
            "response_time": (time.time() - start_time) * 1000,
            "needs_javascript": True,
            "js_detection_reason": f"Erro: {str(e)}",
            "error_details": str(e)
        }
    
    finally:
        if page:
            await browser_pool.release_page(page)

# ========================
# ğŸ¯ ExtraÃ§Ã£o de Dados SEO (COM TITLE V5 INTEGRADO)
# ========================

async def extrair_dados_seo_completos(page: Page, url: str, nivel: int) -> Dict:
    """ğŸ¯ Extrai todos os dados SEO da pÃ¡gina renderizada com Title V5"""
    
    try:
        # ğŸš€ TITLE V5 - Sistema semÃ¢ntico avanÃ§ado
        title = await capturar_title_robusto_v5(page)
        
        # ğŸ“‹ Meta description
        description_elem = await page.query_selector('meta[name="description"], meta[property="og:description"]')
        description = ""
        if description_elem:
            description = await description_elem.get_attribute('content') or ""
        
        # ğŸ”— Canonical
        canonical_elem = await page.query_selector('link[rel="canonical"]')
        canonical = ""
        if canonical_elem:
            canonical = await canonical_elem.get_attribute('href') or ""
        
        # ğŸ·ï¸ Headings (pÃ³s-JS)
        headings_data = {}
        for i in range(1, 7):
            headings = await page.query_selector_all(f'h{i}')
            headings_texts = []
            
            for heading in headings:
                text = await heading.inner_text()
                if text.strip():
                    headings_texts.append(text.strip())
            
            headings_data[f'h{i}'] = len(headings_texts)
            headings_data[f'h{i}_texts'] = headings_texts
        
        # ğŸ¯ Meta tags adicionais
        og_title_elem = await page.query_selector('meta[property="og:title"]')
        og_title = ""
        if og_title_elem:
            og_title = await og_title_elem.get_attribute('content') or ""
        
        twitter_title_elem = await page.query_selector('meta[name="twitter:title"]')
        twitter_title = ""
        if twitter_title_elem:
            twitter_title = await twitter_title_elem.get_attribute('content') or ""
        
        # ğŸ“Š Structured Data (JSON-LD)
        structured_data = []
        json_ld_scripts = await page.query_selector_all('script[type="application/ld+json"]')
        for script in json_ld_scripts:
            try:
                content = await script.inner_text()
                structured_data.append(json.loads(content))
            except:
                pass
        
        return {
            # ğŸ“‹ Dados bÃ¡sicos (compatibilidade)
            "title": title,
            "description": description,
            "canonical": canonical,
            
            # ğŸ·ï¸ Headings
            **headings_data,
            
            # ğŸ†• Meta tags avanÃ§adas
            "og_title": og_title,
            "twitter_title": twitter_title,
            
            # ğŸ“Š Dados estruturados
            "structured_data_count": len(structured_data),
            "has_structured_data": len(structured_data) > 0,
            
            # ğŸ¯ Dados para validaÃ§Ã£o de headings
            "h1_texts": headings_data.get('h1_texts', []),
            "h2_texts": headings_data.get('h2_texts', []),
            "h1_ausente": headings_data.get('h1', 0) == 0,
            "h2_ausente": headings_data.get('h2', 0) == 0,
        }
        
    except Exception as e:
        print(f"âŒ Erro extraindo dados SEO de {url}: {e}")
        return {
            "title": "",
            "description": "",
            "canonical": f"Erro extraindo dados: {str(e)}",
            "h1": 0, "h2": 0, "h3": 0, "h4": 0, "h5": 0, "h6": 0,
            "h1_texts": [], "h2_texts": [],
            "h1_ausente": True, "h2_ausente": True
        }

# ========================
# ğŸš€ TITLE V5 - Sistema SemÃ¢ntico Completo
# ========================

async def capturar_title_robusto_v5(page: Page) -> str:
    """ğŸ¯ Title Capture V5 - Sistema SemÃ¢ntico com Blacklist Inteligente"""
    
    try:
        # 1ï¸âƒ£ CAPTURA INICIAL ROBUSTA
        title = await _capturar_title_basico(page)
        
        # 2ï¸âƒ£ FILTRO SEMÃ‚NTICO V5 - Blacklist de titles inÃºteis
        title_processado = _aplicar_filtro_semantico_v5(title)
        
        # 3ï¸âƒ£ FALLBACK INTELIGENTE se title for inÃºtil
        if _title_e_inutel(title_processado):
            title_alternativo = await _buscar_title_alternativo(page)
            if title_alternativo and not _title_e_inutel(title_alternativo):
                print(f"   ğŸ”„ Title alternativo usado: '{title_alternativo[:50]}...'")
                return title_alternativo
        
        # ğŸ” LOG FINAL
        if title_processado:
            print(f"   âœ… Title V5 capturado: '{title_processado[:50]}{'...' if len(title_processado) > 50 else ''}'")
        else:
            print("   âŒ Title permanece vazio apÃ³s V5")
        
        return title_processado
        
    except Exception as e:
        print(f"   âŒ Erro crÃ­tico Title V5: {e}")
        return ""

async def _capturar_title_basico(page: Page) -> str:
    """ğŸ¯ Captura bÃ¡sica robusta"""
    
    # EstratÃ©gia 1: Wait for function
    try:
        await page.wait_for_function(
            "document.title && document.title.trim().length > 0", 
            timeout=5000
        )
    except:
        pass
    
    # EstratÃ©gia 2: Title API
    title = await page.title()
    
    # EstratÃ©gia 3: DOM direto
    if not title or title.strip() == "":
        try:
            title_element = await page.query_selector('title')
            if title_element:
                title = await title_element.inner_text()
        except:
            pass
    
    # EstratÃ©gia 4: SPA wait + evaluate
    if not title or title.strip() == "" or title.lower() in ['loading', 'carregando', 'app']:
        try:
            await page.wait_for_timeout(2000)
            title = await page.title()
            
            if not title or title.strip() == "":
                title = await page.evaluate("document.title")
        except:
            pass
    
    return title.strip() if title else ""

def _aplicar_filtro_semantico_v5(title: str) -> str:
    """ğŸ¯ Filtro SemÃ¢ntico V5 - Remove titles inÃºteis renderizados"""
    
    if not title:
        return ""
    
    title_clean = title.strip()
    title_lower = title_clean.lower()
    
    # ğŸš« BLACKLIST V5 - Titles inÃºteis pÃ³s-renderizaÃ§Ã£o
    blacklist_titles = {
        # Estados de loading
        'loading', 'carregando', 'loading...', 'carregando...',
        'aguarde', 'wait', 'please wait', 'por favor aguarde',
        
        # Apps genÃ©ricos
        'app', 'react app', 'vue app', 'angular app', 'next app',
        'nextjs app', 'nuxt app', 'gatsby app',
        
        # Placeholders tÃ©cnicos
        'apisanitizer', 'sanitizer', 'api', 'index', 'main',
        'default', 'untitled', 'document', 'new document',
        
        # Estados de erro
        'error', 'erro', 'not found', 'pÃ¡gina nÃ£o encontrada',
        '404', '500', 'server error', 'erro do servidor',
        
        # CMS defaults
        'wordpress', 'wp', 'drupal', 'joomla', 'cms',
        'admin', 'dashboard', 'painel',
        
        # Desenvolvimento
        'localhost', 'development', 'dev', 'test', 'teste',
        'staging', 'beta', 'alpha', 'demo',
        
        # GenÃ©ricos extremos
        'page', 'pÃ¡gina', 'site', 'website', 'web site',
        'home', 'inÃ­cio', 'inicial', 'principal'
    }
    
    # Verifica blacklist exata
    if title_lower in blacklist_titles:
        print(f"   ğŸš« Title blacklisted: '{title_clean}'")
        return ""
    
    # Verifica padrÃµes suspeitos
    if len(title_clean) < 3:
        print(f"   ğŸš« Title muito curto: '{title_clean}'")
        return ""
    
    if not any(c.isalpha() for c in title_clean):
        print(f"   ğŸš« Title sem letras: '{title_clean}'")
        return ""
    
    palavras_suspeitas = ['undefined', 'null', 'nan', '[object object]', 'typeof']
    if any(palavra in title_lower for palavra in palavras_suspeitas):
        print(f"   ğŸš« Title com JS artifacts: '{title_clean}'")
        return ""
    
    if any(char in title_clean for char in ['/', '\\', 'http://', 'https://']):
        print(f"   ğŸš« Title parece URL: '{title_clean}'")
        return ""
    
    return title_clean

def _title_e_inutel(title: str) -> bool:
    """ğŸ” Verifica se title Ã© inÃºtil"""
    if not title or len(title.strip()) < 3:
        return True
    
    title_lower = title.lower().strip()
    
    titles_inuteis = [
        'loading', 'app', 'react app', 'vue app', 'angular app',
        'apisanitizer', 'sanitizer', 'default', 'untitled',
        'document', 'page', 'site', 'website', 'home'
    ]
    
    return title_lower in titles_inuteis

async def _buscar_title_alternativo(page: Page) -> str:
    """ğŸ” Busca titles alternativos quando o principal Ã© inÃºtil"""
    
    try:
        alternativas = []
        
        # 1. H1 principal como title
        h1_elem = await page.query_selector('h1')
        if h1_elem:
            h1_text = await h1_elem.inner_text()
            if h1_text and len(h1_text.strip()) > 3:
                alternativas.append(f"H1: {h1_text.strip()}")
        
        # 2. Meta og:title
        og_title_elem = await page.query_selector('meta[property="og:title"]')
        if og_title_elem:
            og_title = await og_title_elem.get_attribute('content')
            if og_title and len(og_title.strip()) > 3:
                alternativas.append(f"OG: {og_title.strip()}")
        
        # 3. Meta twitter:title
        twitter_title_elem = await page.query_selector('meta[name="twitter:title"]')
        if twitter_title_elem:
            twitter_title = await twitter_title_elem.get_attribute('content')
            if twitter_title and len(twitter_title.strip()) > 3:
                alternativas.append(f"Twitter: {twitter_title.strip()}")
        
        # 4. Primeiro heading nÃ£o-vazio
        for i in range(2, 7):  # h2 atÃ© h6
            heading_elem = await page.query_selector(f'h{i}')
            if heading_elem:
                heading_text = await heading_elem.inner_text()
                if heading_text and len(heading_text.strip()) > 3:
                    alternativas.append(f"H{i}: {heading_text.strip()}")
                    break
        
        # 5. Retorna a melhor alternativa
        if alternativas:
            melhor = alternativas[0]
            tipo, texto = melhor.split(': ', 1)
            print(f"   ğŸ”„ Title alternativo encontrado via {tipo}")
            return texto
        
        return ""
        
    except Exception as e:
        print(f"   âš ï¸ Erro buscando title alternativo: {e}")
        return ""

# ========================
# ğŸ”— ExtraÃ§Ã£o de Links (MANTÃ‰M SEU CÃ“DIGO)
# ========================

async def extrair_links_internos(page: Page, dominio_base: str) -> List[str]:
    """ğŸ”— Extrai links internos da pÃ¡gina (pÃ³s-JS)"""
    
    try:
        links = []
        
        # ğŸ” Busca todos os links
        link_elements = await page.query_selector_all('a[href]')
        
        for link_elem in link_elements:
            href = await link_elem.get_attribute('href')
            
            if href and not href.startswith(('mailto:', 'tel:', 'javascript:', '#')):
                # Resolve URL relativa
                full_url = urljoin(page.url, href)
                parsed = urlparse(full_url)
                
                # SÃ³ links internos
                if parsed.netloc == dominio_base:
                    clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                    if parsed.query:
                        clean_url += f"?{parsed.query}"
                    
                    links.append(clean_url.rstrip('/'))
        
        return list(set(links))  # Remove duplicatas
        
    except Exception as e:
        return []

# ========================
# ğŸ’¾ Cache System (MANTÃ‰M SEU CÃ“DIGO)
# ========================

def salvar_cache_playwright(nome_arquivo: str, dados: List[Dict]):
    """ğŸ’¾ Salva cache compatÃ­vel"""
    with open(nome_arquivo, 'wb') as f:
        pickle.dump(dados, f)

def carregar_cache_playwright(nome_arquivo: str) -> Optional[List[Dict]]:
    """ğŸ“‚ Carrega cache existente"""
    if os.path.exists(nome_arquivo):
        with open(nome_arquivo, 'rb') as f:
            return pickle.load(f)
    return None

def excluir_cache_playwright(nome_arquivo: str):
    """ğŸ—‘ï¸ Remove cache antigo"""
    if os.path.exists(nome_arquivo):
        os.remove(nome_arquivo)
        print(f"ğŸ§¹ Cache Playwright removido: {nome_arquivo}")

# ========================
# ğŸš€ FUNÃ‡ÃƒO PRINCIPAL MODIFICADA COM URL MANAGER SEO
# ========================

async def rastrear_playwright_profundo(
    url_inicial: str,
    max_urls: int = 1000,
    max_depth: int = 3,
    forcar_reindexacao: bool = False,
    browser_pool_size: int = 3,
    perfil_seo: str = 'blog'  # ğŸ†• NOVO PARÃ‚METRO
) -> List[Dict]:
    """ğŸš€ Crawler Playwright Enterprise com URLManager SEO"""
    
    # ğŸ’¾ Sistema de cache
    cache_path = f".cache_{urlparse(url_inicial).netloc.replace('.', '_')}_playwright.pkl"
    
    if forcar_reindexacao:
        excluir_cache_playwright(cache_path)
    
    if not forcar_reindexacao:
        cache = carregar_cache_playwright(cache_path)
        if cache:
            print(f"â™»ï¸ Cache Playwright encontrado: {cache_path}")
            return cache
    
    print(f"ğŸš€ Crawler Playwright Enterprise iniciado!")
    print(f"ğŸ“Š ConfiguraÃ§Ã£o: {max_urls} URLs, profundidade {max_depth}, {browser_pool_size} browsers")
    print(f"ğŸ¯ Perfil SEO: {perfil_seo}")
    print(f"ğŸ¯ Modo: RenderizaÃ§Ã£o completa com JavaScript + URLManager SEO")
    
    # ğŸ†• INICIALIZA URL MANAGER SEO
    dominio_base = urlparse(url_inicial).netloc
    url_manager = URLManagerSEO(dominio_base, max_urls, perfil_seo)
    url_manager.adicionar_url(url_inicial, 0)
    
    resultados = []
    
    async with async_playwright() as playwright:
        # ğŸš€ Inicializa pool de browsers
        browser_pool = BrowserPool(browser_pool_size)
        await browser_pool.initialize(playwright)
        
        try:
            # ğŸ“Š Progress bar assÃ­ncrona
            with tqdm(total=max_urls, desc="ğŸ¯ Playwright SEO Crawling") as pbar:
                
                # ğŸ”„ NOVA LÃ“GICA COM URL MANAGER SEO
                while True:
                    # ObtÃ©m prÃ³xima URL do manager
                    proxima = url_manager.obter_proxima_url()
                    if not proxima:
                        break
                    
                    url_atual, nivel = proxima
                    
                    # ğŸ“¦ Cria lote de URLs para processar em paralelo
                    lote_atual = [(url_atual, nivel)]
                    tasks = []
                    
                    # Adiciona mais URLs ao lote se disponÃ­vel
                    for _ in range(PlaywrightConfig.MAX_CONCURRENT_PAGES - 1):
                        proxima_extra = url_manager.obter_proxima_url()
                        if proxima_extra and len(resultados) + len(lote_atual) < max_urls:
                            lote_atual.append(proxima_extra)
                        else:
                            break
                    
                    # Cria tasks para o lote
                    for url_lote, nivel_lote in lote_atual:
                        task = processar_url_playwright(
                            url_lote, nivel_lote, dominio_base, browser_pool
                        )
                        tasks.append(task)
                    
                    # âš¡ Executa lote em paralelo
                    if tasks:
                        lote_resultados = await asyncio.gather(*tasks, return_exceptions=True)
                        
                        for resultado in lote_resultados:
                            if isinstance(resultado, dict):
                                resultados.append(resultado)
                                
                                # ğŸ”— Adiciona links encontrados ao manager
                                if (resultado.get("links_encontrados") and 
                                    resultado["nivel"] < max_depth):
                                    adicionadas = url_manager.adicionar_lote_urls_seo(
                                        resultado["links_encontrados"], 
                                        resultado["nivel"] + 1, 
                                        resultado["url"]
                                    )
                                    
                                    if adicionadas > 0 and len(resultados) % 50 == 0:
                                        print(f"   ğŸ”— {adicionadas} URLs adicionadas do nÃ­vel {resultado['nivel']+1}")
                                
                                pbar.update(1)
                            else:
                                pbar.update(1)
                    
                    # Log periÃ³dico com estatÃ­sticas SEO
                    if len(resultados) % 50 == 0:
                        relatorio = url_manager.obter_relatorio_seo()
                        print(f"   ğŸ“Š Progresso: {len(resultados)} URLs | Fila: {relatorio['urls_na_fila']}")
                        print(f"      Tipos: {relatorio['distribuicao_por_tipo']}")
        
        finally:
            # ğŸ”š Cleanup
            await browser_pool.close_all()
    
    # ğŸ’¾ Salva cache
    salvar_cache_playwright(cache_path, resultados)
    
    # ğŸ“Š RelatÃ³rio final SEO
    relatorio_final = url_manager.obter_relatorio_seo()
    js_sites = len([r for r in resultados if r.get("needs_javascript", False)])
    
    print(f"\nğŸ“Š RELATÃ“RIO PLAYWRIGHT SEO FINAL:")
    print(f"   Perfil usado: {relatorio_final['perfil_seo']}")
    print(f"   URLs processadas: {relatorio_final['urls_visitadas']}")
    print(f"   URLs descobertas: {relatorio_final['total_descobertas']}")
    print(f"   DistribuiÃ§Ã£o: {relatorio_final['distribuicao_por_tipo']}")
    print(f"   Cobertura: {relatorio_final['cobertura_por_tipo']}")
    print(f"   ğŸ¤– Sites com JS: {js_sites} ({js_sites/len(resultados)*100:.1f}%)")
    
    return resultados

# ========================
# ğŸ§ª FunÃ§Ã£o de Teste
# ========================

async def testar_playwright_crawler():
    """ğŸ§ª Testa o crawler Playwright com URLManager SEO"""
    print("ğŸ§ª Testando Crawler Playwright Enterprise com URLManager SEO...")
    
    # URLs de teste
    urls_teste = [
        "https://example.com",  # Site simples
        "https://gndisul.com.br",  # Site real para teste
    ]
    
    for url in urls_teste:
        print(f"\nğŸ” Testando: {url}")
        resultado = await rastrear_playwright_profundo(
            url, 
            max_urls=10, 
            max_depth=2, 
            perfil_seo='blog'
        )
        
        print(f"ğŸ“Š Resultados obtidos: {len(resultado)}")
        
        if resultado:
            for i, item in enumerate(resultado[:3]):  # Mostra primeiros 3
                print(f"  {i+1}. âœ… {item['url']}")
                print(f"       Title: {item.get('title', 'N/A')[:60]}...")
                print(f"       Status: {item.get('status_code_http', 'N/A')}")
                print(f"       JS Needed: {item.get('needs_javascript', 'N/A')}")
                print(f"       Links: {len(item.get('links_encontrados', []))}")

# ========================
# ğŸ¯ EXECUÃ‡ÃƒO COMPATÃVEL
# ========================

if __name__ == "__main__":
    print("ğŸš€ Testando Crawler Playwright Enterprise v3.0 com URLManager SEO")
    print("ğŸ¯ Funcionalidades:")
    print("   âœ… Browser Pool otimizado")
    print("   âœ… SiteAnalyzer inteligente") 
    print("   âœ… Title V5 com blacklist semÃ¢ntica")
    print("   âœ… URLManager SEO com priorizaÃ§Ã£o")
    print("   âœ… RelatÃ³rios detalhados por tipo")
    
    # ğŸš€ ExecuÃ§Ã£o de teste
    asyncio.run(testar_playwright_crawler())