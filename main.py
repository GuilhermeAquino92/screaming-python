# main.py - Atualizado para detec√ß√£o de CSS que oculta headings

import pandas as pd
import os
from crawler import rastrear_profundo as crawler_requests
# from crawler_selenium import rastrear_selenium_profundo as crawler_selenium  # Mantenha se tiver
from status_checker import verificar_status_http
from metatags import extrair_metatags
from validador_headings import validar_headings  # üÜï AGORA COM DETEC√á√ÉO DE CSS
from http_inseguro import extrair_http_inseguros
from exporters.excel_manager import exportar_relatorio_completo
import warnings

warnings.filterwarnings("ignore")

# ====================
# CONFIGURA√á√ïES
# ====================
url_inicial = "https://ccgsaude.com.br/"
MAX_URLS = 3000
MAX_WORKERS = 20  # Nova configura√ß√£o para threading
MAX_DEPTH = 3     # Nova configura√ß√£o para profundidade
USE_SELENIUM_SE_ERRO = True
FORCAR_REINDEXACAO = False
PASTA_SAIDA = "output"
ARQUIVO_SAIDA = os.path.join(PASTA_SAIDA, "urls_com_status_novo.xlsx")
os.makedirs(PASTA_SAIDA, exist_ok=True)

# ====================
# CRAWLER OTIMIZADO
# ====================
print("üì° Iniciando rastreamento otimizado...")

try:
    urls_com_dados = crawler_requests(
        url_inicial,
        max_urls=MAX_URLS,
        max_depth=MAX_DEPTH,
        max_workers=MAX_WORKERS,
        forcar_reindexacao=FORCAR_REINDEXACAO
    )
    print("‚úÖ Rastreamento com Requests otimizado conclu√≠do.")
except Exception as e:
    print(f"‚ö†Ô∏è Erro com Requests: {e}")
    if USE_SELENIUM_SE_ERRO:
        print("üîÅ Tentando com Selenium...")
        try:
            # Importa selenium crawler se necess√°rio
            from crawler_selenium import rastrear_selenium_profundo as crawler_selenium
            urls_com_dados = crawler_selenium(
                url_inicial,
                max_urls=MAX_URLS,
                forcar_reindexacao=FORCAR_REINDEXACAO
            )
            print("‚úÖ Rastreamento com Selenium conclu√≠do.")
        except ImportError:
            print("‚ùå Crawler Selenium n√£o dispon√≠vel. Continuando com dados parciais do Requests...")
            urls_com_dados = []
        except Exception as selenium_error:
            print(f"‚ùå Erro tamb√©m com Selenium: {selenium_error}")
            urls_com_dados = []
    else:
        print("‚ùå Crawler falhou. Criando lista vazia...")
        urls_com_dados = []

df = pd.DataFrame(urls_com_dados)

# ====================
# COMPATIBILIDADE - RENOMEAR COLUNAS
# ====================
# O crawler otimizado retorna nomes diferentes, vamos padronizar
if 'status_code' not in df.columns and 'status_code' in df.columns:
    df = df.rename(columns={'status_code': 'status_code'})

# Adicionar coluna 'status_code' se n√£o existir (compatibilidade)
if 'status_code' not in df.columns:
    df['status_code'] = None

print(f"üìä DataFrame criado com {len(df)} linhas e colunas: {list(df.columns)}")

# ====================
# FILTRAR APENAS HTML
# ====================
EXTENSOES_INVALIDAS = [
    '.jpg', '.jpeg', '.png', '.gif', '.svg',
    '.webp', '.bmp', '.ico', '.pdf',
    '.doc', '.docx', '.xls', '.xlsx',
    '.zip', '.rar', '.7z', '.js', '.css',
    '.mp3', '.mp4', '.avi', '.mov'
]

print(f"üìù URLs antes do filtro: {len(df)}")
df = df[~df['url'].str.lower().str.contains('|'.join([f'{ext}$' for ext in EXTENSOES_INVALIDAS]), na=False)]
print(f"üìù URLs ap√≥s filtro HTML: {len(df)}")

# ====================
# STATUS HTTP - SEMPRE EXECUTAR PARA AUDITORIA COMPLETA
# ====================
urls_http = df['url'].dropna().unique().tolist()

# SEMPRE verifica status HTTP para auditoria completa, mesmo que o crawler tenha dados
print(f"üîç Verificando status HTTP em {len(urls_http)} URLs √∫nicas (auditoria completa)...")
try:
    df_status = pd.DataFrame(verificar_status_http(urls_http, max_threads=50))
    
    # Se o crawler j√° tinha status, compara e mant√©m o mais confi√°vel
    if 'status_code' in df.columns and not df['status_code'].isna().all():
        print("‚ÑπÔ∏è Comparando status do crawler com verifica√ß√£o independente...")
        df = df.merge(df_status, on='url', how='left', suffixes=('_crawler', '_check'))
        
        # Prioriza verifica√ß√£o independente para auditoria
        df['status_code_final'] = df['status_code_http'].fillna(df['status_code_crawler'])
        df['status_code_http'] = df['status_code_final']
        
        # Remove colunas tempor√°rias
        df = df.drop(columns=['status_code_crawler', 'status_code_check', 'status_code_final'], errors='ignore')
    else:
        df = df.merge(df_status, on='url', how='left')
        
except Exception as e:
    print(f"‚ö†Ô∏è Erro na verifica√ß√£o de status HTTP: {e}")
    # Se falhar, usa dados do crawler se dispon√≠vel
    if 'status_code' in df.columns:
        df['status_code_http'] = df['status_code']
    else:
        df['status_code_http'] = None

# ====================
# METATAGS - PROCESSA TODAS AS URLs (incluindo erros para auditoria SEO)
# ====================
urls_meta = df['url'].dropna().unique().tolist()
print(f"üîé Extraindo metatags de {len(urls_meta)} URLs √∫nicas (incluindo URLs com erro para auditoria completa)...")

if urls_meta:
    df_meta = pd.DataFrame(extrair_metatags(urls_meta, max_threads=50))
    df = df.merge(df_meta, on='url', how='left')
else:
    print("‚ö†Ô∏è Nenhuma URL encontrada para extra√ß√£o de metatags")
    # Adiciona colunas vazias para compatibilidade
    df['title'] = ''
    df['description'] = ''

# ====================
# üÜï HEADINGS COM DETEC√á√ÉO DE CSS - PROCESSA TODAS AS URLs
# ====================
urls_head = df['url'].dropna().unique().tolist()
print(f"üß† Validando headings com detec√ß√£o de CSS em {len(urls_head)} URLs √∫nicas...")
print(f"üîç Detectando: headings vazios, ocultos por CSS (display:none, color:white, etc.)")

if urls_head:
    df_head = pd.DataFrame(validar_headings(urls_head, max_threads=30))  # Reduzido threads para an√°lise CSS
    df = df.merge(df_head, on='url', how='left')
    
    # üÜï LOG ESTAT√çSTICAS DE CSS
    total_urls = len(df_head)
    urls_com_vazios = len([r for r in df_head.to_dict('records') if r.get('tem_headings_vazios', False)])
    urls_com_ocultos = len([r for r in df_head.to_dict('records') if r.get('tem_headings_ocultos', False)])
    total_vazios = sum([r.get('headings_vazios_count', 0) for r in df_head.to_dict('records')])
    total_ocultos = sum([r.get('headings_ocultos_count', 0) for r in df_head.to_dict('records')])
    
    print(f"üìä RELAT√ìRIO DE HEADINGS COM CSS:")
    print(f"   üìù URLs analisadas: {total_urls}")
    print(f"   üï≥Ô∏è URLs com headings vazios: {urls_com_vazios} ({total_vazios} headings)")
    print(f"   üé® URLs com headings ocultos por CSS: {urls_com_ocultos} ({total_ocultos} headings)")
    
    if urls_com_ocultos > 0:
        print(f"   ‚ö†Ô∏è ATEN√á√ÉO: Headings ocultos por CSS podem prejudicar SEO!")
        
else:
    print("‚ö†Ô∏è Nenhuma URL encontrada para valida√ß√£o de headings")

# ====================
# HTTP INSEGURO - PROCESSA TODAS AS URLs (auditoria de seguran√ßa completa)
# ====================
urls_http_inseguro = df['url'].dropna().unique().tolist()
print(f"üö® Buscando http:// inseguro em {len(urls_http_inseguro)} URLs √∫nicas (auditoria de seguran√ßa completa)...")

if urls_http_inseguro:
    df_http = pd.DataFrame(extrair_http_inseguros(urls_http_inseguro, max_threads=40))
else:
    print("‚ö†Ô∏è Nenhuma URL encontrada para verifica√ß√£o de HTTP inseguro")
    df_http = pd.DataFrame(columns=['url'])  # DataFrame vazio

# ====================
# AUDITORIA: TITLES & DESCRIPTIONS - L√ìGICA ORIGINAL PRESERVADA
# ====================
print("üìã Gerando abas de auditoria (l√≥gica original preservada)...")

# Garante que as colunas existem
if 'title' not in df.columns:
    df['title'] = ''
if 'description' not in df.columns:
    df['description'] = ''

# FILTRO ORIGINAL - Remove pagina√ß√£o para auditoria
df_filtrado = df[~df["url"].str.contains(r"\?page=\d+", na=False)].copy()

# L√ìGICA ORIGINAL - Title e Description ausentes
df_title_ausente = df_filtrado[df_filtrado["title"].str.strip() == ""].copy()
df_description_ausente = df_filtrado[df_filtrado["description"].str.strip() == ""].copy()

# L√ìGICA ORIGINAL - Duplicados com value_counts()
title_count = df_filtrado["title"].dropna().str.strip()
desc_count = df_filtrado["description"].dropna().str.strip()

# Remove strings vazias ANTES do value_counts (l√≥gica original)
title_count = title_count[title_count != ""]
desc_count = desc_count[desc_count != ""]

# Value counts para encontrar duplicados (l√≥gica original)
titles_duplicados = title_count.value_counts()
descs_duplicados = desc_count.value_counts()

# Filtra apenas os que aparecem mais de 1 vez (l√≥gica original)
titles_duplicados = titles_duplicados[titles_duplicados > 1].index.tolist()
descs_duplicados = descs_duplicados[descs_duplicados > 1].index.tolist()

# DataFrames de duplicados (l√≥gica original)
df_title_duplicado = df_filtrado[df_filtrado["title"].isin(titles_duplicados)].copy()
df_description_duplicado = df_filtrado[df_filtrado["description"].isin(descs_duplicados)].copy()

# ====================
# ERROS HTTP - L√ìGICA ORIGINAL PRESERVADA
# ====================
# L√ìGICA ORIGINAL: startswith(('3', '4', '5'))
if 'status_code_http' in df.columns:
    df_errors = df[df['status_code_http'].astype(str).str.startswith(('3', '4', '5'))].copy()
    if not df_errors.empty:
        df_errors["tipo_erro"] = df_errors['status_code_http'].astype(str).str[0] + "xx"
else:
    print("‚ö†Ô∏è Coluna status_code_http n√£o encontrada para an√°lise de erros")
    df_errors = pd.DataFrame()

# ====================
# üÜï ESTAT√çSTICAS FINAIS COM CSS
# ====================
print("\nüìä ESTAT√çSTICAS FINAIS - AUDITORIA SEO COMPLETA COM CSS:")
print(f"üìù Total de URLs processadas: {len(df)}")

# Determina qual coluna de status usar
status_col = 'status_code_http' if 'status_code_http' in df.columns else 'status_code'

if status_col in df.columns and not df[status_col].isna().all():
    status_200 = len(df[df[status_col] == 200])
    status_3xx = len(df[df[status_col].astype(str).str.startswith('3')])
    status_4xx = len(df[df[status_col].astype(str).str.startswith('4')])
    status_5xx = len(df[df[status_col].astype(str).str.startswith('5')])
    
    print(f"‚úÖ URLs com status 200: {status_200}")
    print(f"üîÑ URLs com redirecionamento (3xx): {status_3xx}")
    print(f"‚ùå URLs com erro cliente (4xx): {status_4xx}")
    print(f"üö® URLs com erro servidor (5xx): {status_5xx}")
    print(f"üìä Total de URLs com problemas: {len(df_errors)} (importante para SEO!)")
else:
    print("‚ö†Ô∏è Dados de status HTTP n√£o dispon√≠veis")

if 'response_time' in df.columns:
    tempo_medio = df['response_time'].mean()
    print(f"‚ö° Tempo m√©dio de resposta: {tempo_medio:.2f}ms")

print(f"üìã Title ausente: {len(df_title_ausente)} (cr√≠tico para SEO)")
print(f"üìã Description ausente: {len(df_description_ausente)} (importante para SEO)")
print(f"üìã Title duplicado: {len(df_title_duplicado)} (problema de SEO)")
print(f"üìã Description duplicado: {len(df_description_duplicado)} (problema de SEO)")

# üÜï ESTAT√çSTICAS DE HEADINGS COM CSS
print(f"\nüß† AN√ÅLISE DE HEADINGS COM DETEC√á√ÉO DE CSS:")
if 'headings_vazios_count' in df.columns:
    total_vazios_final = df['headings_vazios_count'].sum()
    urls_vazios_final = len(df[df['headings_vazios_count'] > 0])
    print(f"üï≥Ô∏è Headings vazios: {total_vazios_final} em {urls_vazios_final} URLs")

if 'headings_ocultos_count' in df.columns:
    total_ocultos_final = df['headings_ocultos_count'].sum() 
    urls_ocultos_final = len(df[df['headings_ocultos_count'] > 0])
    print(f"üé® Headings ocultos por CSS: {total_ocultos_final} em {urls_ocultos_final} URLs")
    
    if total_ocultos_final > 0:
        print(f"   ‚ö†Ô∏è ALERTA SEO: Headings ocultos por CSS podem ser penalizados pelo Google!")

print(f"üéØ Nova aba 'Headings_Vazios' incluir√° an√°lise detalhada de CSS")

# ====================
# EXPORTA√á√ÉO - ESTRUTURA ORIGINAL PRESERVADA + CSS
# ====================

auditorias = {
    "df_title_ausente": df_title_ausente,
    "df_description_ausente": df_description_ausente,
    "df_title_duplicado": df_title_duplicado,
    "df_description_duplicado": df_description_duplicado,
    "df_errors": df_errors
}

print(f"\nüöÄ Exportando relat√≥rio com NOVA funcionalidade de detec√ß√£o de CSS...")
exportar_relatorio_completo(df, df_http, auditorias, ARQUIVO_SAIDA)