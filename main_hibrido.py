# main_hibrido_enterprise.py - Pipeline SEO Enterprise 3.0 üöÄ
# Arquitetura cir√∫rgica: Orquestrador puro + Engines especializadas

import pandas as pd
import os
import asyncio
import sys
import datetime
from urllib.parse import urlparse

# ========================
# üéØ CONFIGURA√á√ÉO GLOBAL ENTERPRISE
# ========================

URL_BASE = "https://ccgsaude.com.br"
MAX_URLS = 1000
MAX_DEPTH = 3

def gerar_nome_arquivo_seguro(url_base):
    """üîß Gera nome de arquivo seguro"""
    import re
    nome_limpo = url_base.replace('https://', '').replace('http://', '')
    nome_limpo = re.sub(r'[<>:"/\\|?*]', '_', nome_limpo)
    nome_limpo = nome_limpo.replace('.', '_').replace('/', '_')
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    return f"seo_enterprise_{nome_limpo}_{timestamp}.xlsx"

ARQUIVO_SAIDA = gerar_nome_arquivo_seguro(URL_BASE)

# ========================
# üîç IMPORTS DIN√ÇMICOS ENTERPRISE
# ========================

# Crawlers h√≠bridos
try:
    from crawler import rastrear_profundo as crawler_requests
    REQUESTS_AVAILABLE = True
    print("‚úÖ Crawler Requests dispon√≠vel")
except ImportError:
    REQUESTS_AVAILABLE = False
    print("‚ùå Crawler Requests n√£o dispon√≠vel")

try:
    from crawler_playwright import rastrear_playwright_profundo
    PLAYWRIGHT_AVAILABLE = True
    print("‚úÖ Crawler Playwright dispon√≠vel")
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("‚ùå Crawler Playwright n√£o dispon√≠vel")

# Excel Manager Enterprise
try:
    from exporters.excel_manager import exportar_relatorio_completo
    EXCEL_MANAGER_AVAILABLE = True
    print("‚úÖ Excel Manager Enterprise dispon√≠vel")
except ImportError:
    EXCEL_MANAGER_AVAILABLE = False
    print("‚ùå Excel Manager Enterprise n√£o dispon√≠vel")

# SSL Validator Enterprise
try:
    from ssl_problems import validar_ssl_completo
    SSL_VALIDATOR_AVAILABLE = True
    print("‚úÖ SSL Validator Enterprise dispon√≠vel")
except ImportError:
    SSL_VALIDATOR_AVAILABLE = False
    print("‚ùå SSL Validator Enterprise n√£o dispon√≠vel")

# Prioriza√ß√£o Pipeline
try:
    from priorizacao_pipeline import executar_priorizacao_completa
    PRIORIZACAO_AVAILABLE = True
    print("‚úÖ Prioriza√ß√£o Pipeline dispon√≠vel")
except ImportError:
    PRIORIZACAO_AVAILABLE = False
    print("‚ùå Prioriza√ß√£o Pipeline n√£o dispon√≠vel")

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ========================
# üß† DETECTOR JS ENTERPRISE
# ========================

async def detectar_necessidade_js_enterprise(url: str) -> tuple[bool, str, int]:
    """üß† Detec√ß√£o enterprise de necessidade de JS com score"""
    
    try:
        import requests
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        html_lower = response.text.lower()
        
        # Sistema de pontua√ß√£o enterprise
        js_score = 0
        reasons = []
        
        # Frameworks JS (peso alto)
        frameworks = ['react', 'vue', 'angular', 'next.js', 'nuxt', 'svelte']
        if any(fw in html_lower for fw in frameworks):
            js_score += 30
            reasons.append("Framework JS detectado")
        
        # SPA patterns (peso alto)
        spa_patterns = ['data-reactroot', 'ng-app', 'v-app', '__next', '__nuxt']
        if any(pattern in html_lower for pattern in spa_patterns):
            js_score += 25
            reasons.append("SPA pattern detectado")
        
        # Dynamic loading (peso m√©dio)
        loading_indicators = ['loading', 'carregando', 'spinner', 'skeleton']
        if any(word in html_lower for word in loading_indicators):
            js_score += 15
            reasons.append("Loading din√¢mico detectado")
        
        # API calls (peso m√©dio)
        api_patterns = ['fetch(', 'axios', '$.ajax', 'api/', 'graphql', 'json']
        if any(api in html_lower for api in api_patterns):
            js_score += 20
            reasons.append("Chamadas de API detectadas")
        
        # Bundle patterns (peso baixo)
        bundle_patterns = ['webpack', 'bundle.js', 'chunk.js', 'vendor.js']
        if any(bundle in html_lower for bundle in bundle_patterns):
            js_score += 10
            reasons.append("Bundles JS detectados")
        
        # Hydration patterns (peso alto)
        hydration_patterns = ['hydrate', 'ssr', 'server-side']
        if any(hydration in html_lower for hydration in hydration_patterns):
            js_score += 25
            reasons.append("SSR/Hydration detectado")
        
        needs_js = js_score >= 50  # Threshold enterprise
        reason = " | ".join(reasons) if reasons else "Site aparenta ser est√°tico"
        
        return needs_js, reason, js_score
        
    except Exception as e:
        return True, f"Erro na detec√ß√£o: {str(e)} - usando Playwright por seguran√ßa", 100

# ========================
# üîç PR√â-AUDITORIA SSL ENTERPRISE
# ========================

def executar_pre_auditoria_ssl(url_base: str) -> dict:
    """üîç Pr√©-auditoria SSL estrat√©gica antes do crawling"""
    
    print(f"üîç PR√â-AUDITORIA SSL ENTERPRISE")
    print(f"üéØ URL: {url_base}")
    
    resultado_ssl = {
        'ssl_valido': True,
        'problemas_encontrados': [],
        'recomendacoes': [],
        'impacto_crawling': 'baixo'
    }
    
    if SSL_VALIDATOR_AVAILABLE:
        try:
            ssl_resultado = validar_ssl_completo(url_base)
            
            if isinstance(ssl_resultado, dict):
                if not ssl_resultado.get('ssl_valido', True):
                    resultado_ssl['ssl_valido'] = False
                    resultado_ssl['problemas_encontrados'] = ssl_resultado.get('problemas', [])
                    resultado_ssl['impacto_crawling'] = 'alto'
                    
                    print(f"üö® PROBLEMAS SSL DETECTADOS:")
                    for problema in resultado_ssl['problemas_encontrados']:
                        print(f"   ‚ùå {problema}")
                    
                    resultado_ssl['recomendacoes'].append("Corrigir certificado SSL antes do crawling")
                    resultado_ssl['recomendacoes'].append("SSL inv√°lido pode impactar crawl budget")
                else:
                    print(f"‚úÖ SSL v√°lido e seguro")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na valida√ß√£o SSL: {e}")
            resultado_ssl['problemas_encontrados'].append(f"Erro na valida√ß√£o: {e}")
    else:
        print(f"‚ö†Ô∏è SSL Validator n√£o dispon√≠vel - pulando pr√©-auditoria")
    
    return resultado_ssl

# ========================
# üöÄ CRAWLING H√çBRIDO ENTERPRISE
# ========================

async def executar_crawling_hibrido_enterprise():
    """üöÄ Crawling h√≠brido enterprise com detec√ß√£o inteligente"""
    
    print(f"\nüöÄ CRAWLING H√çBRIDO ENTERPRISE")
    print("="*60)
    
    urls_coletadas = []
    metodo_utilizado = "ERRO"
    deteccao_js = {}
    
    # Verifica disponibilidade dos crawlers
    if not REQUESTS_AVAILABLE and not PLAYWRIGHT_AVAILABLE:
        print("‚ùå ERRO CR√çTICO: Nenhum crawler dispon√≠vel!")
        return [], "ERRO", {}
    
    # üß† Detec√ß√£o inteligente se Playwright dispon√≠vel
    if PLAYWRIGHT_AVAILABLE:
        print(f"üß† Executando detec√ß√£o JS enterprise...")
        
        try:
            needs_js, reason, score = await detectar_necessidade_js_enterprise(URL_BASE)
            
            deteccao_js = {
                'needs_js': needs_js,
                'reason': reason,
                'score': score,
                'threshold': 50
            }
            
            print(f"üìä Score JS: {score}/100 (threshold: 50)")
            print(f"üìã M√©todo recomendado: {'Playwright' if needs_js else 'Requests'}")
            print(f"üìù Raz√£o: {reason}")
            
            # Executa crawler recomendado
            if needs_js:
                print(f"\nüé≠ Executando Playwright Enterprise...")
                urls_coletadas = await rastrear_playwright_profundo(
                    URL_BASE,
                    max_urls=MAX_URLS,
                    max_depth=MAX_DEPTH,
                    forcar_reindexacao=False
                )
                metodo_utilizado = "PLAYWRIGHT_ENTERPRISE"
            else:
                if REQUESTS_AVAILABLE:
                    print(f"\n‚ö° Executando Requests Otimizado...")
                    urls_coletadas = crawler_requests(
                        URL_BASE,
                        max_urls=MAX_URLS,
                        max_depth=MAX_DEPTH
                    )
                    metodo_utilizado = "REQUESTS_ENTERPRISE"
                else:
                    print(f"\nüé≠ Requests n√£o dispon√≠vel, usando Playwright...")
                    urls_coletadas = await rastrear_playwright_profundo(
                        URL_BASE,
                        max_urls=MAX_URLS,
                        max_depth=MAX_DEPTH,
                        forcar_reindexacao=False
                    )
                    metodo_utilizado = "PLAYWRIGHT_FALLBACK"
                    
        except Exception as e:
            print(f"‚ùå Erro na detec√ß√£o/execu√ß√£o: {e}")
            metodo_utilizado = "REQUESTS_FALLBACK"
    
    # Fallback para Requests se n√£o conseguiu usar Playwright
    if not urls_coletadas and REQUESTS_AVAILABLE:
        print(f"\n‚ö° Executando Requests (fallback)...")
        try:
            urls_coletadas = crawler_requests(
                URL_BASE,
                max_urls=MAX_URLS,
                max_depth=MAX_DEPTH
            )
            metodo_utilizado = "REQUESTS_FALLBACK"
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico no Requests: {e}")
            return [], "ERRO", deteccao_js
    
    print(f"‚úÖ Crawling conclu√≠do: {len(urls_coletadas)} URLs")
    print(f"üéØ M√©todo utilizado: {metodo_utilizado}")
    
    return urls_coletadas, metodo_utilizado, deteccao_js

# ========================
# üìä AN√ÅLISE DE DADOS ENTERPRISE
# ========================

def analisar_distribuicao_urls_enterprise(df):
    """üìä An√°lise enterprise de distribui√ß√£o de URLs"""
    
    if df.empty:
        return {}
    
    print(f"\nüìä AN√ÅLISE ENTERPRISE DE URLs")
    
    tipos_url = {}
    
    for _, row in df.iterrows():
        url = row.get('url', '')
        if not url:
            continue
        
        path = urlparse(url).path.lower()
        
        # Classifica√ß√£o enterprise mais detalhada
        if path in ['', '/']:
            tipo = 'homepage'
        elif any(termo in path for termo in ['/blog', '/post', '/artigo', '/news', '/noticia']):
            tipo = 'conteudo'
        elif any(termo in path for termo in ['/produto', '/product', '/item']):
            tipo = 'produto'
        elif any(termo in path for termo in ['/categoria', '/category', '/cat']):
            tipo = 'categoria'
        elif any(termo in path for termo in ['/sobre', '/contato', '/servicos', '/empresa']):
            tipo = 'institucional'
        elif any(termo in path for termo in ['/api', '/feed', '/rss', '/sitemap']):
            tipo = 'api_feed'
        elif any(termo in path for termo in ['/admin', '/login', '/dashboard']):
            tipo = 'administrativo'
        elif path.endswith(('.pdf', '.doc', '.xls', '.zip')):
            tipo = 'arquivo'
        else:
            tipo = 'outros'
        
        tipos_url[tipo] = tipos_url.get(tipo, 0) + 1
    
    # Log enterprise com insights
    total = sum(tipos_url.values())
    print(f"üìà Total analisado: {total} URLs")
    
    icones = {
        'homepage': 'üè†', 'conteudo': 'üìù', 'produto': 'üõí', 
        'categoria': 'üìÅ', 'institucional': 'üè¢', 'outros': 'üìÑ',
        'api_feed': 'üîó', 'administrativo': '‚öôÔ∏è', 'arquivo': 'üìé'
    }
    
    for tipo, count in sorted(tipos_url.items(), key=lambda x: x[1], reverse=True):
        percent = (count / total) * 100 if total > 0 else 0
        icone = icones.get(tipo, 'üìÑ')
        print(f"   {icone} {tipo.capitalize()}: {count} ({percent:.1f}%)")
    
    # Insights autom√°ticos
    if tipos_url.get('conteudo', 0) > total * 0.3:
        print(f"üí° Site com foco em conte√∫do detectado")
    if tipos_url.get('produto', 0) > total * 0.2:
        print(f"üí° E-commerce detectado")
    if tipos_url.get('api_feed', 0) > 0:
        print(f"üí° APIs/Feeds detectados - verificar indexabilidade")
    
    return tipos_url

# ========================
# üß† INTELIG√äNCIA ESTRAT√âGICA ENTERPRISE
# ========================

def executar_inteligencia_estrategica_enterprise(arquivo_final: str):
    """üß† Executa intelig√™ncia estrat√©gica enterprise"""
    
    print(f"\nüß† INTELIG√äNCIA ESTRAT√âGICA ENTERPRISE")
    print("="*60)
    
    resultados = {
        'backlog_gerado': False,
        'backlog_path': None,
        'insights': []
    }
    
    if PRIORIZACAO_AVAILABLE:
        try:
            if os.path.exists(arquivo_final):
                print(f"üîç Analisando: {os.path.basename(arquivo_final)}")
                
                # Executa prioriza√ß√£o enterprise
                backlog_path = executar_priorizacao_completa(arquivo_final)
                
                if backlog_path and os.path.exists(backlog_path):
                    resultados['backlog_gerado'] = True
                    resultados['backlog_path'] = backlog_path
                    resultados['insights'].append("Backlog estrat√©gico gerado com sucesso")
                    
                    print(f"‚úÖ BACKLOG ESTRAT√âGICO ENTERPRISE GERADO!")
                    print(f"üìÅ Arquivo: {os.path.basename(backlog_path)}")
                    print(f"üìä Cont√©m: Problemas priorizados + Resumo executivo")
                    print(f"üéØ Ready para: Stakeholders, Dev team, Cliente")
                else:
                    resultados['insights'].append("Falha na gera√ß√£o do backlog")
                    print(f"‚ö†Ô∏è Backlog n√£o foi gerado")
            
        except Exception as e:
            resultados['insights'].append(f"Erro na prioriza√ß√£o: {str(e)}")
            print(f"‚ùå Erro na prioriza√ß√£o: {e}")
    else:
        resultados['insights'].append("M√≥dulo de prioriza√ß√£o n√£o dispon√≠vel")
        print(f"‚ö†Ô∏è M√≥dulo de prioriza√ß√£o n√£o dispon√≠vel")
    
    return resultados

# ========================
# üéØ PIPELINE PRINCIPAL ENTERPRISE
# ========================

async def main_pipeline_enterprise():
    """üéØ Pipeline principal enterprise - orquestrador puro"""
    
    print("üéØ PIPELINE SEO ENTERPRISE 3.0")
    print("="*60)
    print(f"üåê URL: {URL_BASE}")
    print(f"üìä Max URLs: {MAX_URLS}")
    print(f"üìè Max Depth: {MAX_DEPTH}")
    print(f"üìÅ Arquivo: {ARQUIVO_SAIDA}")
    
    # Status dos m√≥dulos
    print(f"\nüîß STATUS DOS M√ìDULOS:")
    print(f"   Requests: {'‚úÖ' if REQUESTS_AVAILABLE else '‚ùå'}")
    print(f"   Playwright: {'‚úÖ' if PLAYWRIGHT_AVAILABLE else '‚ùå'}")
    print(f"   Excel Manager: {'‚úÖ' if EXCEL_MANAGER_AVAILABLE else '‚ùå'}")
    print(f"   SSL Validator: {'‚úÖ' if SSL_VALIDATOR_AVAILABLE else '‚ùå'}")
    print(f"   Prioriza√ß√£o: {'‚úÖ' if PRIORIZACAO_AVAILABLE else '‚ùå'}")
    
    # üîç FASE 1: PR√â-AUDITORIA SSL
    print(f"\nüîç FASE 1: PR√â-AUDITORIA SSL ENTERPRISE")
    resultado_ssl = executar_pre_auditoria_ssl(URL_BASE)
    
    # üöÄ FASE 2: CRAWLING H√çBRIDO
    print(f"\nüöÄ FASE 2: CRAWLING H√çBRIDO ENTERPRISE")
    urls_coletadas, metodo_utilizado, deteccao_js = await executar_crawling_hibrido_enterprise()
    
    if not urls_coletadas:
        print("‚ùå ERRO CR√çTICO: Nenhuma URL coletada!")
        sys.exit(1)
    
    # Cria DataFrame enterprise
    df_enterprise = pd.DataFrame(urls_coletadas)
    print(f"üìä DataFrame Enterprise: {len(df_enterprise)} URLs, {len(df_enterprise.columns)} colunas")
    
    # üìä FASE 3: AN√ÅLISE DE DADOS ENTERPRISE
    tipos_url = analisar_distribuicao_urls_enterprise(df_enterprise)
    
    # üè∑Ô∏è FASE 4: METADADOS ENTERPRISE (TIPAGEM GARANTIDA)
    print(f"\nüè∑Ô∏è FASE 4: ADICIONANDO METADADOS ENTERPRISE")
    
    # üîß NORMALIZA√á√ÉO ENTERPRISE DE TIPOS
    def normalizar_metadados_enterprise(df, metodo, deteccao_js, resultado_ssl):
        """üîß Normaliza metadados enterprise com tipagem garantida"""
        
        print(f"üîß Normalizando metadados enterprise...")
        
        # Metadados b√°sicos do pipeline (SEMPRE STRING)
        df['crawler_method'] = str(metodo if metodo else 'UNKNOWN')
        df['pipeline_version'] = str('enterprise_3.0')
        df['analysis_timestamp'] = str(datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        
        # SSL Status (SEMPRE STRING)
        ssl_valido = resultado_ssl.get('ssl_valido', True) if isinstance(resultado_ssl, dict) else True
        df['ssl_status'] = str('valid' if ssl_valido else 'invalid')
        
        # Metadados JS Detection (SEMPRE STRING)
        if deteccao_js and isinstance(deteccao_js, dict):
            # JS Score como string para evitar problemas de tipo
            js_score = deteccao_js.get('score', 0)
            df['js_score'] = str(int(js_score) if isinstance(js_score, (int, float)) else 0)
            
            # JS Reason como string limpa
            js_reason = deteccao_js.get('reason', 'N/A')
            df['js_detection_reason'] = str(js_reason if js_reason else 'N/A')
        else:
            df['js_score'] = str('0')
            df['js_detection_reason'] = str('N/A')
        
        # Metadados de controle de qualidade
        df['data_quality_check'] = str('passed')
        df['excel_manager_ready'] = str('true')
        
        print(f"‚úÖ Tipagem enterprise garantida:")
        print(f"   üìä crawler_method: {type(df['crawler_method'].iloc[0]).__name__}")
        print(f"   üìä pipeline_version: {type(df['pipeline_version'].iloc[0]).__name__}")
        print(f"   üìä analysis_timestamp: {type(df['analysis_timestamp'].iloc[0]).__name__}")
        print(f"   üìä ssl_status: {type(df['ssl_status'].iloc[0]).__name__}")
        print(f"   üìä js_score: {type(df['js_score'].iloc[0]).__name__}")
        print(f"   üìä js_detection_reason: {type(df['js_detection_reason'].iloc[0]).__name__}")
        
        return df
    
    # Aplica normaliza√ß√£o enterprise
    df_enterprise = normalizar_metadados_enterprise(
        df_enterprise, 
        metodo_utilizado, 
        deteccao_js, 
        resultado_ssl
    )
    
    print(f"‚úÖ Metadados enterprise normalizados e tipagem garantida")
    
    # üì§ FASE 5: EXPORTA√á√ÉO ENTERPRISE
    print(f"\nüì§ FASE 5: EXPORTA√á√ÉO ENTERPRISE")
    
    if EXCEL_MANAGER_AVAILABLE:
        try:
            print(f"üî• Utilizando Excel Manager Enterprise...")
            print(f"   ‚úÖ Engines cir√∫rgicas integradas")
            print(f"   ‚úÖ Detec√ß√£o autom√°tica de problemas")
            print(f"   ‚úÖ Zero falsos positivos")
            
            # O Excel Manager cuida de TODAS as auditorias via engines cir√∫rgicas
            arquivo_final = exportar_relatorio_completo(
                df_enterprise, 
                pd.DataFrame(),  # HTTP inseguro ser√° processado pelas engines
                {},  # Auditorias ser√£o feitas pelas engines
                ARQUIVO_SAIDA
            )
            
            print(f"‚úÖ Relat√≥rio Enterprise exportado: {arquivo_final}")
            
        except Exception as e:
            print(f"‚ùå Erro na exporta√ß√£o enterprise: {e}")
            
            # Fallback b√°sico
            arquivo_csv = ARQUIVO_SAIDA.replace('.xlsx', '_fallback.csv')
            arquivo_csv = os.path.join(os.getcwd(), os.path.basename(arquivo_csv))
            df_enterprise.to_csv(arquivo_csv, index=False, encoding='utf-8')
            print(f"üîÑ Fallback CSV: {arquivo_csv}")
            arquivo_final = arquivo_csv
    else:
        # Exporta√ß√£o b√°sica
        print(f"üìä Usando exporta√ß√£o b√°sica...")
        arquivo_final = os.path.join(os.getcwd(), os.path.basename(ARQUIVO_SAIDA))
        df_enterprise.to_excel(arquivo_final, index=False)
        print(f"‚úÖ Arquivo b√°sico exportado: {arquivo_final}")
    
    # üìä FASE 6: ESTAT√çSTICAS FINAIS ENTERPRISE
    print(f"\nüìä FASE 6: ESTAT√çSTICAS ENTERPRISE")
    print(f"üéØ M√©todo utilizado: {metodo_utilizado}")
    print(f"üìù URLs processadas: {len(df_enterprise)}")
    print(f"üìà Tipos de p√°gina: {len(tipos_url)}")
    print(f"üîç SSL status: {'‚úÖ V√°lido' if resultado_ssl['ssl_valido'] else '‚ùå Inv√°lido'}")
    
    if deteccao_js:
        print(f"üß† JS Score: {deteccao_js.get('score', 0)}/100")
    
    # Retorna arquivo final para pr√≥xima fase
    return arquivo_final

# ========================
# üîÑ WRAPPER ENTERPRISE
# ========================

def main_enterprise():
    """üîÑ Wrapper principal enterprise"""
    
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    try:
        # Executa pipeline enterprise
        arquivo_final = asyncio.run(main_pipeline_enterprise())
        
        # üß† INTELIG√äNCIA ESTRAT√âGICA
        if arquivo_final and os.path.exists(arquivo_final):
            resultados_ia = executar_inteligencia_estrategica_enterprise(arquivo_final)
            
            # üéâ RELAT√ìRIO FINAL ENTERPRISE
            print(f"\n" + "="*80)
            print(f"üéâ PIPELINE SEO ENTERPRISE 3.0 CONCLU√çDO!")
            print(f"="*80)
            print(f"üî• DELIVERABLES ENTERPRISE:")
            print(f"   1. üìä Relat√≥rio T√©cnico Completo: {os.path.basename(arquivo_final)}")
            
            if resultados_ia['backlog_gerado']:
                print(f"   2. üß† Backlog Estrat√©gico: {os.path.basename(resultados_ia['backlog_path'])}")
            
            print(f"\nüíé DIFERENCIAIS ENTERPRISE:")
            print(f"   ‚úÖ Pr√©-auditoria SSL autom√°tica")
            print(f"   ‚úÖ Detec√ß√£o JS inteligente com score")
            print(f"   ‚úÖ Engines cir√∫rgicas (zero falsos positivos)")
            print(f"   ‚úÖ An√°lise enterprise de tipos de p√°gina")
            print(f"   ‚úÖ Metadados completos de rastreabilidade")
            print(f"   ‚úÖ Prioriza√ß√£o autom√°tica de problemas")
            
            print(f"\nüöÄ STATUS: READY FOR ENTERPRISE DEPLOYMENT!")
            print(f"="*80)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Pipeline cancelado pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro cr√≠tico enterprise: {e}")
        import traceback
        traceback.print_exc()
        
        # Modo de recupera√ß√£o enterprise
        print(f"\nüö® MODO DE RECUPERA√á√ÉO ENTERPRISE...")
        try:
            if REQUESTS_AVAILABLE:
                urls_recuperacao = crawler_requests(URL_BASE, min(MAX_URLS, 100), 2)
                if urls_recuperacao:
                    df_recuperacao = pd.DataFrame(urls_recuperacao)
                    arquivo_recuperacao = gerar_nome_arquivo_seguro(URL_BASE).replace('seo_enterprise_', 'recovery_')
                    arquivo_recuperacao = os.path.join(os.getcwd(), arquivo_recuperacao)
                    df_recuperacao.to_excel(arquivo_recuperacao, index=False)
                    print(f"‚úÖ Relat√≥rio de recupera√ß√£o: {arquivo_recuperacao}")
        except Exception as e2:
            print(f"üí• Falha total na recupera√ß√£o: {e2}")

# ========================
# üöÄ ENTRY POINT ENTERPRISE
# ========================

if __name__ == "__main__":
    print("üöÄ INICIANDO PIPELINE SEO ENTERPRISE 3.0")
    print("Arquitetura: Orquestrador Puro + Engines Cir√∫rgicas")
    print("="*60)
    main_enterprise()