# main_hibrido_enterprise.py - Pipeline SEO Enterprise 3.0 ğŸš€
# Arquitetura cirÃºrgica: Orquestrador puro + Engines especializadas

import pandas as pd
import os
import asyncio
import sys
import datetime
from urllib.parse import urlparse

# ========================
# ğŸ¯ CONFIGURAÃ‡ÃƒO GLOBAL ENTERPRISE
# ========================

URL_BASE = "https://ccgsaude.com.br"
MAX_URLS = 1000
MAX_DEPTH = 3

def gerar_nome_arquivo_seguro(url_base):
    """ğŸ”§ Gera nome de arquivo seguro"""
    import re
    nome_limpo = url_base.replace('https://', '').replace('http://', '')
    nome_limpo = re.sub(r'[<>:"/\\|?*]', '_', nome_limpo)
    nome_limpo = nome_limpo.replace('.', '_').replace('/', '_')
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    return f"seo_enterprise_{nome_limpo}_{timestamp}.xlsx"

ARQUIVO_SAIDA = gerar_nome_arquivo_seguro(URL_BASE)

# ========================
# ğŸ” IMPORTS DINÃ‚MICOS ENTERPRISE
# ========================

# Crawlers hÃ­bridos
try:
    from crawler import rastrear_profundo as crawler_requests
    REQUESTS_AVAILABLE = True
    print("âœ… Crawler Requests disponÃ­vel")
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âŒ Crawler Requests nÃ£o disponÃ­vel")

try:
    from crawler_playwright import rastrear_playwright_profundo
    PLAYWRIGHT_AVAILABLE = True
    print("âœ… Crawler Playwright disponÃ­vel")
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âŒ Crawler Playwright nÃ£o disponÃ­vel")

# Excel Manager Enterprise
try:
    from exporters.excel_manager import exportar_relatorio_completo
    EXCEL_MANAGER_AVAILABLE = True
    print("âœ… Excel Manager Enterprise disponÃ­vel")
except ImportError:
    EXCEL_MANAGER_AVAILABLE = False
    print("âŒ Excel Manager Enterprise nÃ£o disponÃ­vel")

# SSL Validator Enterprise
try:
    from ssl_problems import validar_ssl_completo
    SSL_VALIDATOR_AVAILABLE = True
    print("âœ… SSL Validator Enterprise disponÃ­vel")
except ImportError:
    SSL_VALIDATOR_AVAILABLE = False
    print("âŒ SSL Validator Enterprise nÃ£o disponÃ­vel")

# PriorizaÃ§Ã£o Pipeline
try:
    from priorizacao_pipeline import executar_priorizacao_completa
    PRIORIZACAO_AVAILABLE = True
    print("âœ… PriorizaÃ§Ã£o Pipeline disponÃ­vel")
except ImportError:
    PRIORIZACAO_AVAILABLE = False
    print("âŒ PriorizaÃ§Ã£o Pipeline nÃ£o disponÃ­vel")

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ========================
# ğŸ§  DETECTOR JS ENTERPRISE
# ========================

async def detectar_necessidade_js_enterprise(url: str) -> tuple[bool, str, int]:
    """ğŸ§  DetecÃ§Ã£o enterprise de necessidade de JS com score"""
    
    try:
        import requests
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        html_lower = response.text.lower()
        
        # Sistema de pontuaÃ§Ã£o enterprise
        js_score = 0
        reasons = []
        
        # Frameworks JS (peso alto)
        frameworks = ['react', 'vue', 'angular', 'next.js', 'nuxt', 'svelte']
        if any(fw in html_lower for fw in frameworks):
            js_score += 30
            reasons.append("Framework JS detectado")
        
        # SPA patterns (peso alto)
        spa_patterns = ['data-reactroot', 'ng-app', 'v-app', '__next', '__nuxt']
        if any(pattern in html_lower for pattern in spa_patterns):
            js_score += 25
            reasons.append("SPA pattern detectado")
        
        # Dynamic loading (peso mÃ©dio)
        loading_indicators = ['loading', 'carregando', 'spinner', 'skeleton']
        if any(word in html_lower for word in loading_indicators):
            js_score += 15
            reasons.append("Loading dinÃ¢mico detectado")
        
        # API calls (peso mÃ©dio)
        api_patterns = ['fetch(', 'axios', '$.ajax', 'api/', 'graphql', 'json']
        if any(api in html_lower for api in api_patterns):
            js_score += 20
            reasons.append("Chamadas de API detectadas")
        
        # Bundle patterns (peso baixo)
        bundle_patterns = ['webpack', 'bundle.js', 'chunk.js', 'vendor.js']
        if any(bundle in html_lower for bundle in bundle_patterns):
            js_score += 10
            reasons.append("Bundles JS detectados")
        
        # Hydration patterns (peso alto)
        hydration_patterns = ['hydrate', 'ssr', 'server-side']
        if any(hydration in html_lower for hydration in hydration_patterns):
            js_score += 25
            reasons.append("SSR/Hydration detectado")
        
        needs_js = js_score >= 50  # Threshold enterprise
        reason = " | ".join(reasons) if reasons else "Site aparenta ser estÃ¡tico"
        
        return needs_js, reason, js_score
        
    except Exception as e:
        return True, f"Erro na detecÃ§Ã£o: {str(e)} - usando Playwright por seguranÃ§a", 100

# ========================
# ğŸ” PRÃ‰-AUDITORIA SSL ENTERPRISE
# ========================

def executar_pre_auditoria_ssl(url_base: str) -> dict:
    """ğŸ” PrÃ©-auditoria SSL estratÃ©gica antes do crawling"""
    
    print(f"ğŸ” PRÃ‰-AUDITORIA SSL ENTERPRISE")
    print(f"ğŸ¯ URL: {url_base}")
    
    resultado_ssl = {
        'ssl_valido': True,
        'problemas_encontrados': [],
        'recomendacoes': [],
        'impacto_crawling': 'baixo'
    }
    
    if SSL_VALIDATOR_AVAILABLE:
        try:
            ssl_resultado = validar_ssl_completo(url_base)
            
            if isinstance(ssl_resultado, dict):
                if not ssl_resultado.get('ssl_valido', True):
                    resultado_ssl['ssl_valido'] = False
                    resultado_ssl['problemas_encontrados'] = ssl_resultado.get('problemas', [])
                    resultado_ssl['impacto_crawling'] = 'alto'
                    
                    print(f"ğŸš¨ PROBLEMAS SSL DETECTADOS:")
                    for problema in resultado_ssl['problemas_encontrados']:
                        print(f"   âŒ {problema}")
                    
                    resultado_ssl['recomendacoes'].append("Corrigir certificado SSL antes do crawling")
                    resultado_ssl['recomendacoes'].append("SSL invÃ¡lido pode impactar crawl budget")
                else:
                    print(f"âœ… SSL vÃ¡lido e seguro")
            
        except Exception as e:
            print(f"âš ï¸ Erro na validaÃ§Ã£o SSL: {e}")
            resultado_ssl['problemas_encontrados'].append(f"Erro na validaÃ§Ã£o: {e}")
    else:
        print(f"âš ï¸ SSL Validator nÃ£o disponÃ­vel - pulando prÃ©-auditoria")
    
    return resultado_ssl

# ========================
# ğŸš€ CRAWLING HÃBRIDO ENTERPRISE
# ========================

async def executar_crawling_hibrido_enterprise():
    """ğŸš€ Crawling hÃ­brido enterprise com detecÃ§Ã£o inteligente"""
    
    print(f"\nğŸš€ CRAWLING HÃBRIDO ENTERPRISE")
    print("="*60)
    
    urls_coletadas = []
    metodo_utilizado = "ERRO"
    deteccao_js = {}
    
    # Verifica disponibilidade dos crawlers
    if not REQUESTS_AVAILABLE and not PLAYWRIGHT_AVAILABLE:
        print("âŒ ERRO CRÃTICO: Nenhum crawler disponÃ­vel!")
        return [], "ERRO", {}
    
    # ğŸ§  DetecÃ§Ã£o inteligente se Playwright disponÃ­vel
    if PLAYWRIGHT_AVAILABLE:
        print(f"ğŸ§  Executando detecÃ§Ã£o JS enterprise...")
        
        try:
            needs_js, reason, score = await detectar_necessidade_js_enterprise(URL_BASE)
            
            deteccao_js = {
                'needs_js': needs_js,
                'reason': reason,
                'score': score,
                'threshold': 50
            }
            
            print(f"ğŸ“Š Score JS: {score}/100 (threshold: 50)")
            print(f"ğŸ“‹ MÃ©todo recomendado: {'Playwright' if needs_js else 'Requests'}")
            print(f"ğŸ“ RazÃ£o: {reason}")
            
            # Executa crawler recomendado
            if needs_js:
                print(f"\nğŸ­ Executando Playwright Enterprise...")
                urls_coletadas = await rastrear_playwright_profundo(
                    URL_BASE,
                    max_urls=MAX_URLS,
                    max_depth=MAX_DEPTH,
                    forcar_reindexacao=False
                )
                metodo_utilizado = "PLAYWRIGHT_ENTERPRISE"
            else:
                if REQUESTS_AVAILABLE:
                    print(f"\nâš¡ Executando Requests Otimizado...")
                    urls_coletadas = crawler_requests(
                        URL_BASE,
                        max_urls=MAX_URLS,
                        max_depth=MAX_DEPTH
                    )
                    metodo_utilizado = "REQUESTS_ENTERPRISE"
                else:
                    print(f"\nğŸ­ Requests nÃ£o disponÃ­vel, usando Playwright...")
                    urls_coletadas = await rastrear_playwright_profundo(
                        URL_BASE,
                        max_urls=MAX_URLS,
                        max_depth=MAX_DEPTH,
                        forcar_reindexacao=False
                    )
                    metodo_utilizado = "PLAYWRIGHT_FALLBACK"
                    
        except Exception as e:
            print(f"âŒ Erro na detecÃ§Ã£o/execuÃ§Ã£o: {e}")
            metodo_utilizado = "REQUESTS_FALLBACK"
    
    # Fallback para Requests se nÃ£o conseguiu usar Playwright
    if not urls_coletadas and REQUESTS_AVAILABLE:
        print(f"\nâš¡ Executando Requests (fallback)...")
        try:
            urls_coletadas = crawler_requests(
                URL_BASE,
                max_urls=MAX_URLS,
                max_depth=MAX_DEPTH
            )
            metodo_utilizado = "REQUESTS_FALLBACK"
        except Exception as e:
            print(f"âŒ Erro crÃ­tico no Requests: {e}")
            return [], "ERRO", deteccao_js
    
    print(f"âœ… Crawling concluÃ­do: {len(urls_coletadas)} URLs")
    print(f"ğŸ¯ MÃ©todo utilizado: {metodo_utilizado}")
    
    return urls_coletadas, metodo_utilizado, deteccao_js

# ========================
# ğŸ“Š ANÃLISE DE DADOS ENTERPRISE
# ========================

def analisar_distribuicao_urls_enterprise(df):
    """ğŸ“Š AnÃ¡lise enterprise de distribuiÃ§Ã£o de URLs"""
    
    if df.empty:
        return {}
    
    print(f"\nğŸ“Š ANÃLISE ENTERPRISE DE URLs")
    
    tipos_url = {}
    
    for _, row in df.iterrows():
        url = row.get('url', '')
        if not url:
            continue
        
        path = urlparse(url).path.lower()
        
        # ClassificaÃ§Ã£o enterprise mais detalhada
        if path in ['', '/']:
            tipo = 'homepage'
        elif any(termo in path for termo in ['/blog', '/post', '/artigo', '/news', '/noticia']):
            tipo = 'conteudo'
        elif any(termo in path for termo in ['/produto', '/product', '/item']):
            tipo = 'produto'
        elif any(termo in path for termo in ['/categoria', '/category', '/cat']):
            tipo = 'categoria'
        elif any(termo in path for termo in ['/sobre', '/contato', '/servicos', '/empresa']):
            tipo = 'institucional'
        elif any(termo in path for termo in ['/api', '/feed', '/rss', '/sitemap']):
            tipo = 'api_feed'
        elif any(termo in path for termo in ['/admin', '/login', '/dashboard']):
            tipo = 'administrativo'
        elif path.endswith(('.pdf', '.doc', '.xls', '.zip')):
            tipo = 'arquivo'
        else:
            tipo = 'outros'
        
        tipos_url[tipo] = tipos_url.get(tipo, 0) + 1
    
    # Log enterprise com insights
    total = sum(tipos_url.values())
    print(f"ğŸ“ˆ Total analisado: {total} URLs")
    
    icones = {
        'homepage': 'ğŸ ', 'conteudo': 'ğŸ“', 'produto': 'ğŸ›’', 
        'categoria': 'ğŸ“', 'institucional': 'ğŸ¢', 'outros': 'ğŸ“„',
        'api_feed': 'ğŸ”—', 'administrativo': 'âš™ï¸', 'arquivo': 'ğŸ“'
    }
    
    for tipo, count in sorted(tipos_url.items(), key=lambda x: x[1], reverse=True):
        percent = (count / total) * 100 if total > 0 else 0
        icone = icones.get(tipo, 'ğŸ“„')
        print(f"   {icone} {tipo.capitalize()}: {count} ({percent:.1f}%)")
    
    # Insights automÃ¡ticos
    if tipos_url.get('conteudo', 0) > total * 0.3:
        print(f"ğŸ’¡ Site com foco em conteÃºdo detectado")
    if tipos_url.get('produto', 0) > total * 0.2:
        print(f"ğŸ’¡ E-commerce detectado")
    if tipos_url.get('api_feed', 0) > 0:
        print(f"ğŸ’¡ APIs/Feeds detectados - verificar indexabilidade")
    
    return tipos_url

# ========================
# ğŸ§  INTELIGÃŠNCIA ESTRATÃ‰GICA ENTERPRISE
# ========================

def executar_inteligencia_estrategica_enterprise(arquivo_final: str):
    """ğŸ§  Executa inteligÃªncia estratÃ©gica enterprise"""
    
    print(f"\nğŸ§  INTELIGÃŠNCIA ESTRATÃ‰GICA ENTERPRISE")
    print("="*60)
    
    resultados = {
        'backlog_gerado': False,
        'backlog_path': None,
        'insights': []
    }
    
    if PRIORIZACAO_AVAILABLE:
        try:
            if os.path.exists(arquivo_final):
                print(f"ğŸ” Analisando: {os.path.basename(arquivo_final)}")
                
                # Executa priorizaÃ§Ã£o enterprise
                backlog_path = executar_priorizacao_completa(arquivo_final)
                
                if backlog_path and os.path.exists(backlog_path):
                    resultados['backlog_gerado'] = True
                    resultados['backlog_path'] = backlog_path
                    resultados['insights'].append("Backlog estratÃ©gico gerado com sucesso")
                    
                    print(f"âœ… BACKLOG ESTRATÃ‰GICO ENTERPRISE GERADO!")
                    print(f"ğŸ“ Arquivo: {os.path.basename(backlog_path)}")
                    print(f"ğŸ“Š ContÃ©m: Problemas priorizados + Resumo executivo")
                    print(f"ğŸ¯ Ready para: Stakeholders, Dev team, Cliente")
                else:
                    resultados['insights'].append("Falha na geraÃ§Ã£o do backlog")
                    print(f"âš ï¸ Backlog nÃ£o foi gerado")
            
        except Exception as e:
            resultados['insights'].append(f"Erro na priorizaÃ§Ã£o: {str(e)}")
            print(f"âŒ Erro na priorizaÃ§Ã£o: {e}")
    else:
        resultados['insights'].append("MÃ³dulo de priorizaÃ§Ã£o nÃ£o disponÃ­vel")
        print(f"âš ï¸ MÃ³dulo de priorizaÃ§Ã£o nÃ£o disponÃ­vel")
    
    return resultados

# ========================
# ğŸ¯ PIPELINE PRINCIPAL ENTERPRISE
# ========================

async def main_pipeline_enterprise():
    """ğŸ¯ Pipeline principal enterprise - orquestrador puro"""
    
    print("ğŸ¯ PIPELINE SEO ENTERPRISE 3.0")
    print("="*60)
    print(f"ğŸŒ URL: {URL_BASE}")
    print(f"ğŸ“Š Max URLs: {MAX_URLS}")
    print(f"ğŸ“ Max Depth: {MAX_DEPTH}")
    print(f"ğŸ“ Arquivo: {ARQUIVO_SAIDA}")
    
    # Status dos mÃ³dulos
    print(f"\nğŸ”§ STATUS DOS MÃ“DULOS:")
    print(f"   Requests: {'âœ…' if REQUESTS_AVAILABLE else 'âŒ'}")
    print(f"   Playwright: {'âœ…' if PLAYWRIGHT_AVAILABLE else 'âŒ'}")
    print(f"   Excel Manager: {'âœ…' if EXCEL_MANAGER_AVAILABLE else 'âŒ'}")
    print(f"   SSL Validator: {'âœ…' if SSL_VALIDATOR_AVAILABLE else 'âŒ'}")
    print(f"   PriorizaÃ§Ã£o: {'âœ…' if PRIORIZACAO_AVAILABLE else 'âŒ'}")
    
    # ğŸ” FASE 1: PRÃ‰-AUDITORIA SSL
    print(f"\nğŸ” FASE 1: PRÃ‰-AUDITORIA SSL ENTERPRISE")
    resultado_ssl = executar_pre_auditoria_ssl(URL_BASE)
    
    # ğŸš€ FASE 2: CRAWLING HÃBRIDO
    print(f"\nğŸš€ FASE 2: CRAWLING HÃBRIDO ENTERPRISE")
    urls_coletadas, metodo_utilizado, deteccao_js = await executar_crawling_hibrido_enterprise()
    
    if not urls_coletadas:
        print("âŒ ERRO CRÃTICO: Nenhuma URL coletada!")
        sys.exit(1)
    
    # Cria DataFrame enterprise
    df_enterprise = pd.DataFrame(urls_coletadas)
    print(f"ğŸ“Š DataFrame Enterprise: {len(df_enterprise)} URLs, {len(df_enterprise.columns)} colunas")
    
    # ğŸ“Š FASE 3: ANÃLISE DE DADOS ENTERPRISE
    tipos_url = analisar_distribuicao_urls_enterprise(df_enterprise)
    
    # ğŸ·ï¸ FASE 4: METADADOS ENTERPRISE (TIPAGEM GARANTIDA)
    print(f"\nğŸ·ï¸ FASE 4: ADICIONANDO METADADOS ENTERPRISE")
    
    # ğŸ”§ NORMALIZAÃ‡ÃƒO ENTERPRISE DE TIPOS
    def normalizar_metadados_enterprise(df, metodo, deteccao_js, resultado_ssl):
        """ğŸ”§ Normaliza metadados enterprise com tipagem garantida"""
        
        print(f"ğŸ”§ Normalizando metadados enterprise...")
        
        # Metadados bÃ¡sicos do pipeline (SEMPRE STRING)
        df['crawler_method'] = str(metodo if metodo else 'UNKNOWN')
        df['pipeline_version'] = str('enterprise_3.0')
        df['analysis_timestamp'] = str(datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        
        # SSL Status (SEMPRE STRING)
        ssl_valido = resultado_ssl.get('ssl_valido', True) if isinstance(resultado_ssl, dict) else True
        df['ssl_status'] = str('valid' if ssl_valido else 'invalid')
        
        # Metadados JS Detection (SEMPRE STRING)
        if deteccao_js and isinstance(deteccao_js, dict):
            # JS Score como string para evitar problemas de tipo
            js_score = deteccao_js.get('score', 0)
            df['js_score'] = str(int(js_score) if isinstance(js_score, (int, float)) else 0)
            
            # JS Reason como string limpa
            js_reason = deteccao_js.get('reason', 'N/A')
            df['js_detection_reason'] = str(js_reason if js_reason else 'N/A')
        else:
            df['js_score'] = str('0')
            df['js_detection_reason'] = str('N/A')
        
        # Metadados de controle de qualidade
        df['data_quality_check'] = str('passed')
        df['excel_manager_ready'] = str('true')
        
        print(f"âœ… Tipagem enterprise garantida:")
        print(f"   ğŸ“Š crawler_method: {type(df['crawler_method'].iloc[0]).__name__}")
        print(f"   ğŸ“Š pipeline_version: {type(df['pipeline_version'].iloc[0]).__name__}")
        print(f"   ğŸ“Š analysis_timestamp: {type(df['analysis_timestamp'].iloc[0]).__name__}")
        print(f"   ğŸ“Š ssl_status: {type(df['ssl_status'].iloc[0]).__name__}")
        print(f"   ğŸ“Š js_score: {type(df['js_score'].iloc[0]).__name__}")
        print(f"   ğŸ“Š js_detection_reason: {type(df['js_detection_reason'].iloc[0]).__name__}")
        
        return df
    
    # Aplica normalizaÃ§Ã£o enterprise
    df_enterprise = normalizar_metadados_enterprise(
        df_enterprise, 
        metodo_utilizado, 
        deteccao_js, 
        resultado_ssl
    )
    
    print(f"âœ… Metadados enterprise normalizados e tipagem garantida")
    
    # ğŸ“¤ FASE 5: EXPORTAÃ‡ÃƒO ENTERPRISE
    print(f"\nğŸ“¤ FASE 5: EXPORTAÃ‡ÃƒO ENTERPRISE")
    
    if EXCEL_MANAGER_AVAILABLE:
        try:
            print(f"ğŸ”¥ Utilizando Excel Manager Enterprise...")
            print(f"   âœ… Engines cirÃºrgicas integradas")
            print(f"   âœ… DetecÃ§Ã£o automÃ¡tica de problemas")
            print(f"   âœ… Zero falsos positivos")
            
            # O Excel Manager cuida de TODAS as auditorias via engines cirÃºrgicas
            arquivo_final = exportar_relatorio_completo(
                df_enterprise, 
                pd.DataFrame(),  # HTTP inseguro serÃ¡ processado pelas engines
                {},  # Auditorias serÃ£o feitas pelas engines
                ARQUIVO_SAIDA
            )
            
            print(f"âœ… RelatÃ³rio Enterprise exportado: {arquivo_final}")
            
        except Exception as e:
            print(f"âŒ Erro na exportaÃ§Ã£o enterprise: {e}")
            
            # Fallback bÃ¡sico
            arquivo_csv = ARQUIVO_SAIDA.replace('.xlsx', '_fallback.csv')
            arquivo_csv = os.path.join(os.getcwd(), os.path.basename(arquivo_csv))
            df_enterprise.to_csv(arquivo_csv, index=False, encoding='utf-8')
            print(f"ğŸ”„ Fallback CSV: {arquivo_csv}")
            arquivo_final = arquivo_csv
    else:
        # ExportaÃ§Ã£o bÃ¡sica
        print(f"ğŸ“Š Usando exportaÃ§Ã£o bÃ¡sica...")
        arquivo_final = os.path.join(os.getcwd(), os.path.basename(ARQUIVO_SAIDA))
        df_enterprise.to_excel(arquivo_final, index=False)
        print(f"âœ… Arquivo bÃ¡sico exportado: {arquivo_final}")
    
    # ğŸ“Š FASE 6: ESTATÃSTICAS FINAIS ENTERPRISE
    print(f"\nğŸ“Š FASE 6: ESTATÃSTICAS ENTERPRISE")
    print(f"ğŸ¯ MÃ©todo utilizado: {metodo_utilizado}")
    print(f"ğŸ“ URLs processadas: {len(df_enterprise)}")
    print(f"ğŸ“ˆ Tipos de pÃ¡gina: {len(tipos_url)}")
    print(f"ğŸ” SSL status: {'âœ… VÃ¡lido' if resultado_ssl['ssl_valido'] else 'âŒ InvÃ¡lido'}")
    
    if deteccao_js:
        print(f"ğŸ§  JS Score: {deteccao_js.get('score', 0)}/100")
    
    # Retorna arquivo final para prÃ³xima fase
    return arquivo_final

# ========================
# ğŸ”„ WRAPPER ENTERPRISE
# ========================

def main_enterprise():
    """ğŸ”„ Wrapper principal enterprise"""
    
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    try:
        # Executa pipeline enterprise
        arquivo_final = asyncio.run(main_pipeline_enterprise())
        
        # ğŸ§  INTELIGÃŠNCIA ESTRATÃ‰GICA
        if arquivo_final and os.path.exists(arquivo_final):
            resultados_ia = executar_inteligencia_estrategica_enterprise(arquivo_final)
            
            # ğŸ‰ RELATÃ“RIO FINAL ENTERPRISE
            print(f"\n" + "="*80)
            print(f"ğŸ‰ PIPELINE SEO ENTERPRISE 3.0 CONCLUÃDO!")
            print(f"="*80)
            print(f"ğŸ”¥ DELIVERABLES ENTERPRISE:")
            print(f"   1. ğŸ“Š RelatÃ³rio TÃ©cnico Completo: {os.path.basename(arquivo_final)}")
            
            if resultados_ia['backlog_gerado']:
                print(f"   2. ğŸ§  Backlog EstratÃ©gico: {os.path.basename(resultados_ia['backlog_path'])}")
            
            print(f"\nğŸ’ DIFERENCIAIS ENTERPRISE:")
            print(f"   âœ… PrÃ©-auditoria SSL automÃ¡tica")
            print(f"   âœ… DetecÃ§Ã£o JS inteligente com score")
            print(f"   âœ… Engines cirÃºrgicas (zero falsos positivos)")
            print(f"   âœ… AnÃ¡lise enterprise de tipos de pÃ¡gina")
            print(f"   âœ… Metadados completos de rastreabilidade")
            print(f"   âœ… PriorizaÃ§Ã£o automÃ¡tica de problemas")
            
            print(f"\nğŸš€ STATUS: READY FOR ENTERPRISE DEPLOYMENT!")
            print(f"="*80)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Pipeline cancelado pelo usuÃ¡rio")
    except Exception as e:
        print(f"\nâŒ Erro crÃ­tico enterprise: {e}")
        import traceback
        traceback.print_exc()
        
        # Modo de recuperaÃ§Ã£o enterprise
        print(f"\nğŸš¨ MODO DE RECUPERAÃ‡ÃƒO ENTERPRISE...")
        try:
            if REQUESTS_AVAILABLE:
                urls_recuperacao = crawler_requests(URL_BASE, min(MAX_URLS, 100), 2)
                if urls_recuperacao:
                    df_recuperacao = pd.DataFrame(urls_recuperacao)
                    arquivo_recuperacao = gerar_nome_arquivo_seguro(URL_BASE).replace('seo_enterprise_', 'recovery_')
                    arquivo_recuperacao = os.path.join(os.getcwd(), arquivo_recuperacao)
                    df_recuperacao.to_excel(arquivo_recuperacao, index=False)
                    print(f"âœ… RelatÃ³rio de recuperaÃ§Ã£o: {arquivo_recuperacao}")
        except Exception as e2:
            print(f"ğŸ’¥ Falha total na recuperaÃ§Ã£o: {e2}")

# ========================
# ğŸš€ ENTRY POINT ENTERPRISE
# ========================

if __name__ == "__main__":
    print("ğŸš€ INICIANDO PIPELINE SEO ENTERPRISE 3.0")
    print("Arquitetura: Orquestrador Puro + Engines CirÃºrgicas")
    print("="*60)
    main_enterprise()