# main_hibrido.py - Sistema H√≠brido Inteligente com Playwright Enterprise

import pandas as pd
import os
import asyncio
import sys
from urllib.parse import urlparse
from crawler import rastrear_profundo as crawler_requests
from status_checker import verificar_status_http
from metatags import extrair_metatags
from validador_headings import validar_headings
from http_inseguro import extrair_http_inseguros
from exporters.excel_manager import exportar_relatorio_completo
import warnings

# üÜï Import Playwright (com fallback)
try:
    from crawler_playwright import rastrear_playwright_profundo
    PLAYWRIGHT_AVAILABLE = True
    print("‚úÖ Playwright Enterprise dispon√≠vel")
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("‚ö†Ô∏è Playwright n√£o instalado. Modo h√≠brido desabilitado.")
    print("   Para instalar: pip install playwright && playwright install chromium")

warnings.filterwarnings("ignore")

# ========================
# üéØ CONFIGURA√á√ïES H√çBRIDAS
# ========================
url_inicial = "https://gndisul.com.br/"
MAX_URLS = 3000
MAX_WORKERS = 20
MAX_DEPTH = 3
FORCAR_REINDEXACAO = False
PASTA_SAIDA = "output"
ARQUIVO_SAIDA = os.path.join(PASTA_SAIDA, "relatorio_seo_hibrido.xlsx")
os.makedirs(PASTA_SAIDA, exist_ok=True)

# üß† CONFIGURA√á√ïES INTELIGENTES
MODO_CRAWLER = "AUTO"  # AUTO, REQUESTS, PLAYWRIGHT, HIBRIDO
LIMITE_JS_DETECTION = 50  # URLs para testar se precisa JS
FALLBACK_ATIVO = True  # Se Playwright falhar, usa Requests
USE_SELENIUM_SE_ERRO = True  # Selenium como √∫ltimo recurso

# ========================
# ü§ñ Detec√ß√£o Inteligente de Site
# ========================
async def detectar_necessidade_js(url_inicial, limite=50):
    """üß† Testa amostra de URLs para ver se site precisa de JavaScript"""
    
    if not PLAYWRIGHT_AVAILABLE:
        return False, "Playwright n√£o dispon√≠vel"
    
    print(f"üîç Testando se site precisa de JavaScript renderiza√ß√£o...")
    print(f"üìä Analisando at√© {limite} URLs de amostra...")
    
    try:
        # üöÄ Crawl pequeno com Playwright para teste
        amostra_resultados = await rastrear_playwright_profundo(
            url_inicial,
            max_urls=limite,
            max_depth=2,
            browser_pool_size=2
        )
        
        if not amostra_resultados:
            return False, "Nenhuma URL coletada para an√°lise"
        
        # üìä Analisa resultados
        total_urls = len(amostra_resultados)
        urls_com_js = len([r for r in amostra_resultados if r.get("needs_javascript", False)])
        percentual_js = (urls_com_js / total_urls * 100) if total_urls > 0 else 0
        
        # üîç Coleta raz√µes detalhadas
        razoes_js = []
        for resultado in amostra_resultados:
            if resultado.get("needs_javascript", False):
                razao = resultado.get("js_detection_reason", "Desconhecido")
                if razao not in razoes_js:
                    razoes_js.append(razao)
        
        # üéØ Decis√£o inteligente
        precisa_js = percentual_js >= 30  # Se 30%+ das p√°ginas precisam JS
        
        print(f"üìä RESULTADO DA AN√ÅLISE:")
        print(f"   üî¢ URLs testadas: {total_urls}")
        print(f"   ü§ñ URLs que precisam JS: {urls_com_js} ({percentual_js:.1f}%)")
        print(f"   üìã Principais raz√µes: {', '.join(razoes_js[:3])}")
        print(f"   üéØ Decis√£o: {'PRECISA JavaScript' if precisa_js else 'Sites est√°ticos OK'}")
        
        razao_final = f"{percentual_js:.1f}% das p√°ginas precisam JS. Raz√µes: {', '.join(razoes_js[:2])}"
        
        return precisa_js, razao_final
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erro na detec√ß√£o JS: {e}")
        return True, f"Erro na detec√ß√£o, usando Playwright por seguran√ßa: {str(e)}"

# ========================
# üöÄ Sistema de Crawler H√≠brido
# ========================
async def executar_crawler_hibrido():
    """üéØ Sistema inteligente que escolhe melhor crawler"""
    
    print(f"üöÄ Sistema H√≠brido Inteligente Iniciado!")
    print(f"üéØ URL: {url_inicial}")
    print(f"üìä Configura√ß√£o: {MAX_URLS} URLs, profundidade {MAX_DEPTH}")
    
    urls_com_dados = []
    metodo_usado = "INDEFINIDO"
    
    # üß† MODO AUTOM√ÅTICO - Detec√ß√£o Inteligente
    if MODO_CRAWLER == "AUTO" and PLAYWRIGHT_AVAILABLE:
        print(f"\nüß† MODO AUTO: Detectando melhor crawler...")
        
        try:
            precisa_js, razao = await detectar_necessidade_js(url_inicial, LIMITE_JS_DETECTION)
            
            if precisa_js:
                print(f"üéØ DECIS√ÉO: Usar Playwright Enterprise (renderiza√ß√£o JS necess√°ria)")
                print(f"üìù Raz√£o: {razao}")
                metodo_escolhido = "PLAYWRIGHT"
            else:
                print(f"üéØ DECIS√ÉO: Usar Requests Otimizado (site est√°tico detectado)")
                print(f"üìù Raz√£o: {razao}")
                metodo_escolhido = "REQUESTS"
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na detec√ß√£o autom√°tica: {e}")
            print(f"üîÑ Fallback: Usando Playwright por seguran√ßa")
            metodo_escolhido = "PLAYWRIGHT"
    else:
        metodo_escolhido = MODO_CRAWLER
    
    # üöÄ EXECU√á√ÉO DO CRAWLER ESCOLHIDO
    if metodo_escolhido == "PLAYWRIGHT" and PLAYWRIGHT_AVAILABLE:
        try:
            print(f"\nüé≠ Executando Playwright Enterprise...")
            urls_com_dados = await rastrear_playwright_profundo(
                url_inicial,
                max_urls=MAX_URLS,
                max_depth=MAX_DEPTH,
                forcar_reindexacao=FORCAR_REINDEXACAO,
                browser_pool_size=3
            )
            metodo_usado = "PLAYWRIGHT"
            print("‚úÖ Playwright Enterprise conclu√≠do com sucesso!")
            
        except Exception as e:
            print(f"‚ùå Erro com Playwright: {e}")
            if FALLBACK_ATIVO:
                print(f"üîÑ Executando fallback para Requests...")
                metodo_escolhido = "REQUESTS"
            else:
                raise e
    
    if metodo_escolhido == "REQUESTS":
        try:
            print(f"\n‚ö° Executando Requests Otimizado...")
            urls_com_dados = crawler_requests(
                url_inicial,
                max_urls=MAX_URLS,
                max_depth=MAX_DEPTH,
                max_workers=MAX_WORKERS,
                forcar_reindexacao=FORCAR_REINDEXACAO
            )
            metodo_usado = "REQUESTS"
            print("‚úÖ Requests Otimizado conclu√≠do com sucesso!")
            
        except Exception as e:
            print(f"‚ùå Erro com Requests: {e}")
            if FALLBACK_ATIVO and USE_SELENIUM_SE_ERRO:
                print(f"üîÑ Tentando Selenium como √∫ltimo recurso...")
                try:
                    from crawler_selenium import rastrear_selenium_profundo
                    urls_com_dados = rastrear_selenium_profundo(
                        url_inicial,
                        max_urls=MAX_URLS,
                        forcar_reindexacao=FORCAR_REINDEXACAO
                    )
                    metodo_usado = "SELENIUM"
                    print("‚úÖ Selenium conclu√≠do com sucesso!")
                except Exception as selenium_error:
                    print(f"‚ùå Erro tamb√©m com Selenium: {selenium_error}")
                    urls_com_dados = []
                    metodo_usado = "ERRO"
            else:
                urls_com_dados = []
                metodo_usado = "ERRO"
    
    # üéØ MODO H√çBRIDO - Usa ambos para compara√ß√£o
    if metodo_escolhido == "HIBRIDO" and PLAYWRIGHT_AVAILABLE:
        print(f"\nüîÑ MODO H√çBRIDO: Executando Requests + Playwright...")
        
        try:
            # Executa ambos em paralelo (cuidado com recursos)
            print(f"‚ö° Executando Requests primeiro...")
            urls_requests = crawler_requests(
                url_inicial,
                max_urls=MAX_URLS//2,  # Divide URLs
                max_depth=MAX_DEPTH,
                max_workers=MAX_WORKERS//2,
                forcar_reindexacao=FORCAR_REINDEXACAO
            )
            
            print(f"üé≠ Executando Playwright para compara√ß√£o...")
            urls_playwright = await rastrear_playwright_profundo(
                url_inicial,
                max_urls=MAX_URLS//2,
                max_depth=MAX_DEPTH,
                forcar_reindexacao=FORCAR_REINDEXACAO,
                browser_pool_size=2
            )
            
            # Combina resultados (prioriza Playwright para dados SEO)
            urls_dict = {}
            
            # Adiciona dados do Requests
            for item in urls_requests:
                url = item.get('url')
                if url:
                    urls_dict[url] = item
            
            # Atualiza/sobrescreve com dados do Playwright (mais precisos)
            for item in urls_playwright:
                url = item.get('url')
                if url:
                    if url in urls_dict:
                        # Mant√©m dados do Requests, atualiza com dados SEO do Playwright
                        urls_dict[url].update({
                            k: v for k, v in item.items() 
                            if k in ['title', 'description', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
                                   'needs_javascript', 'structured_data_count']
                        })
                    else:
                        urls_dict[url] = item
            
            urls_com_dados = list(urls_dict.values())
            metodo_usado = "H√çBRIDO"
            print(f"‚úÖ Modo H√≠brido conclu√≠do! Combinados {len(urls_dict)} URLs √∫nicos")
            
        except Exception as e:
            print(f"‚ùå Erro no modo h√≠brido: {e}")
            print(f"üîÑ Fallback para Requests apenas...")
            urls_com_dados = crawler_requests(
                url_inicial,
                max_urls=MAX_URLS,
                max_depth=MAX_DEPTH,
                max_workers=MAX_WORKERS,
                forcar_reindexacao=FORCAR_REINDEXACAO
            )
            metodo_usado = "REQUESTS_FALLBACK"
    
    return urls_com_dados, metodo_usado

# ========================
# üîç Processamento de Dados H√≠brido
# ========================
def processar_dados_hibridos(df, metodo_usado):
    """üéØ Processa dados considerando o m√©todo usado"""
    
    print(f"\nüìä Processando dados coletados via {metodo_usado}...")
    
    # ‚úÖ COMPATIBILIDADE - Normaliza nomes de colunas
    colunas_mapping = {
        'status_code': 'status_code_http',  # Padroniza para auditoria
    }
    
    for old_col, new_col in colunas_mapping.items():
        if old_col in df.columns and new_col not in df.columns:
            df = df.rename(columns={old_col: new_col})
    
    # üéØ DADOS ESPEC√çFICOS DO PLAYWRIGHT
    if metodo_usado in ["PLAYWRIGHT", "H√çBRIDO"]:
        print(f"üé≠ Dados Playwright detectados - processamento avan√ßado...")
        
        # Estat√≠sticas de JavaScript
        if 'needs_javascript' in df.columns:
            total_js = len(df[df['needs_javascript'] == True])
            total_estatico = len(df[df['needs_javascript'] == False])
            print(f"   ü§ñ Sites com JS: {total_js}")
            print(f"   üìÑ Sites est√°ticos: {total_estatico}")
        
        # Structured Data
        if 'structured_data_count' in df.columns:
            com_structured = len(df[df['structured_data_count'] > 0])
            print(f"   üìä P√°ginas com dados estruturados: {com_structured}")
        
        # Console Errors
        if 'console_errors' in df.columns:
            com_erros = len(df[df['console_errors'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)])
            print(f"   üö® P√°ginas com erros JavaScript: {com_erros}")
    
    # üîç DADOS ESPEC√çFICOS DO REQUESTS
    elif metodo_usado == "REQUESTS":
        print(f"‚ö° Dados Requests detectados - processamento otimizado...")
        
        if 'response_time' in df.columns:
            tempo_medio = df['response_time'].mean()
            print(f"   ‚è±Ô∏è Tempo m√©dio de resposta: {tempo_medio:.2f}ms")
    
    return df

# ========================
# üéØ MAIN ASS√çNCRONO H√çBRIDO
# ========================
async def main_hibrido():
    """üöÄ Fun√ß√£o principal h√≠brida ass√≠ncrona"""
    
    print("=" * 60)
    print("üöÄ SISTEMA SEO H√çBRIDO ENTERPRISE")
    print("=" * 60)
    
    # üîç FASE 1: Crawling H√≠brido Inteligente
    print(f"\nüì° FASE 1: CRAWLING H√çBRIDO INTELIGENTE")
    urls_com_dados, metodo_usado = await executar_crawler_hibrido()
    
    if not urls_com_dados:
        print("‚ùå ERRO CR√çTICO: Nenhuma URL coletada!")
        sys.exit(1)
    
    df = pd.DataFrame(urls_com_dados)
    print(f"üìä DataFrame criado: {len(df)} URLs, colunas: {list(df.columns)}")
    
    # üéØ FASE 2: Processamento de Dados
    print(f"\nüîß FASE 2: PROCESSAMENTO DE DADOS")
    df = processar_dados_hibridos(df, metodo_usado)
    
    # üö´ FASE 3: Filtros de Qualidade
    print(f"\nüéØ FASE 3: FILTROS DE QUALIDADE")
    EXTENSOES_INVALIDAS = [
        '.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.bmp', '.ico',
        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.7z',
        '.js', '.css', '.mp3', '.mp4', '.avi', '.mov'
    ]
    
    urls_antes = len(df)
    df = df[~df['url'].str.lower().str.contains('|'.join([f'{ext}$' for ext in EXTENSOES_INVALIDAS]), na=False)]
    urls_depois = len(df)
    print(f"üìù Filtro HTML aplicado: {urls_antes} ‚Üí {urls_depois} URLs (-{urls_antes-urls_depois} arquivos)")
    
    # üîç FASE 4: Verifica√ß√µes Adicionais (apenas se necess√°rio)
    print(f"\nüîç FASE 4: VERIFICA√á√ïES ADICIONAIS")
    
    # Status HTTP - s√≥ se Playwright n√£o coletou ou dados inconsistentes
    if metodo_usado != "PLAYWRIGHT" or 'status_code_http' not in df.columns:
        print(f"üîç Verificando status HTTP independente...")
        urls_http = df['url'].dropna().unique().tolist()
        df_status = pd.DataFrame(verificar_status_http(urls_http, max_threads=50))
        df = df.merge(df_status, on='url', how='left', suffixes=('', '_check'))
        
        # Usa verifica√ß√£o independente se n√£o tinha dados
        if 'status_code_http' not in df.columns and 'status_code_http_check' in df.columns:
            df['status_code_http'] = df['status_code_http_check']
    else:
        print(f"‚úÖ Status HTTP j√° coletado via {metodo_usado}")
    
    # Metatags - s√≥ se Playwright n√£o coletou dados completos
    if metodo_usado != "PLAYWRIGHT" or 'title' not in df.columns:
        print(f"üîé Extraindo metatags independente...")
        urls_meta = df['url'].dropna().unique().tolist()
        df_meta = pd.DataFrame(extrair_metatags(urls_meta, max_threads=50))
        df = df.merge(df_meta, on='url', how='left', suffixes=('_pw', ''))
        
        # Prioriza dados do Playwright se existirem
        for col in ['title', 'description']:
            if f'{col}_pw' in df.columns:
                df[col] = df[f'{col}_pw'].fillna(df[col])
                df = df.drop(f'{col}_pw', axis=1)
    else:
        print(f"‚úÖ Metatags j√° coletadas via {metodo_usado}")
    
    # Headings - SEMPRE executar para an√°lise CSS detalhada
    print(f"üß† Validando headings com detec√ß√£o CSS...")
    urls_head = df['url'].dropna().unique().tolist()
    df_head = pd.DataFrame(validar_headings(urls_head, max_threads=30))
    df = df.merge(df_head, on='url', how='left', suffixes=('_pw', '_css'))
    
    # Combina dados de heading quando dispon√≠vel
    for col in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        if f'{col}_pw' in df.columns and f'{col}_css' in df.columns:
            # Prioriza dados do CSS analyzer (mais preciso para an√°lise)
            df[col] = df[f'{col}_css'].fillna(df[f'{col}_pw'])
        elif f'{col}_css' in df.columns:
            df[col] = df[f'{col}_css']
        elif f'{col}_pw' in df.columns:
            df[col] = df[f'{col}_pw']
    
    # Limpa colunas tempor√°rias
    colunas_temp = [col for col in df.columns if col.endswith('_pw') or col.endswith('_css')]
    df = df.drop(colunas_temp, axis=1, errors='ignore')
    
    # HTTP Inseguro
    print(f"üö® Analisando HTTP inseguro...")
    urls_http_inseguro = df['url'].dropna().unique().tolist()
    df_http = pd.DataFrame(extrair_http_inseguros(urls_http_inseguro, max_threads=40))
    
    # üìã FASE 5: Auditorias SEO
    print(f"\nüìã FASE 5: AUDITORIAS SEO")
    
    # Garante colunas necess√°rias
    for col in ['title', 'description']:
        if col not in df.columns:
            df[col] = ''
    
    # Filtro para auditoria (remove pagina√ß√£o)
    df_filtrado = df[~df["url"].str.contains(r"\?page=\d+", na=False)].copy()
    
    # Auditorias originais
    df_title_ausente = df_filtrado[df_filtrado["title"].str.strip() == ""].copy()
    df_description_ausente = df_filtrado[df_filtrado["description"].str.strip() == ""].copy()
    
    # Duplicados
    title_count = df_filtrado["title"].dropna().str.strip()
    desc_count = df_filtrado["description"].dropna().str.strip()
    title_count = title_count[title_count != ""]
    desc_count = desc_count[desc_count != ""]
    
    titles_duplicados = title_count.value_counts()
    descs_duplicados = desc_count.value_counts()
    titles_duplicados = titles_duplicados[titles_duplicados > 1].index.tolist()
    descs_duplicados = descs_duplicados[descs_duplicados > 1].index.tolist()
    
    df_title_duplicado = df_filtrado[df_filtrado["title"].isin(titles_duplicados)].copy()
    df_description_duplicado = df_filtrado[df_filtrado["description"].isin(descs_duplicados)].copy()
    
    # Erros HTTP
    status_col = 'status_code_http' if 'status_code_http' in df.columns else 'status_code'
    if status_col in df.columns:
        df_errors = df[df[status_col].astype(str).str.startswith(('3', '4', '5'))].copy()
        if not df_errors.empty:
            df_errors["tipo_erro"] = df_errors[status_col].astype(str).str[0] + "xx"
    else:
        df_errors = pd.DataFrame()
    
    # üìä FASE 6: Estat√≠sticas Finais
    print(f"\nüìä FASE 6: ESTAT√çSTICAS FINAIS")
    print(f"üéØ M√©todo usado: {metodo_usado}")
    print(f"üìù Total de URLs: {len(df)}")
    
    if status_col in df.columns:
        status_200 = len(df[df[status_col] == 200])
        status_3xx = len(df[df[status_col].astype(str).str.startswith('3')])
        status_4xx = len(df[df[status_col].astype(str).str.startswith('4')])
        status_5xx = len(df[df[status_col].astype(str).str.startswith('5')])
        
        print(f"‚úÖ Status 200: {status_200}")
        print(f"üîÑ Redirecionamentos: {status_3xx}")
        print(f"‚ùå Erros 4xx: {status_4xx}")
        print(f"üö® Erros 5xx: {status_5xx}")
    
    print(f"üìã Title ausente: {len(df_title_ausente)}")
    print(f"üìã Description ausente: {len(df_description_ausente)}")
    print(f"üîÑ Title duplicado: {len(df_title_duplicado)}")
    print(f"üîÑ Description duplicado: {len(df_description_duplicado)}")
    
    # Estat√≠sticas de headings
    if 'headings_vazios_count' in df.columns:
        total_vazios = df['headings_vazios_count'].sum()
        urls_vazios = len(df[df['headings_vazios_count'] > 0])
        print(f"üï≥Ô∏è Headings vazios: {total_vazios} em {urls_vazios} URLs")
    
    if 'headings_ocultos_count' in df.columns:
        total_ocultos = df['headings_ocultos_count'].sum()
        urls_ocultos = len(df[df['headings_ocultos_count'] > 0])
        print(f"üé® Headings ocultos CSS: {total_ocultos} em {urls_ocultos} URLs")
    
    # üìä FASE 7: Exporta√ß√£o
    print(f"\nüöÄ FASE 7: EXPORTA√á√ÉO DO RELAT√ìRIO")
    
    auditorias = {
        "df_title_ausente": df_title_ausente,
        "df_description_ausente": df_description_ausente,
        "df_title_duplicado": df_title_duplicado,
        "df_description_duplicado": df_description_duplicado,
        "df_errors": df_errors
    }
    
    # Adiciona metadados do crawler usado
    df['crawler_method'] = metodo_usado
    df['crawler_version'] = 'H√≠brido v2.0'
    
    exportar_relatorio_completo(df, df_http, auditorias, ARQUIVO_SAIDA)
    
    print(f"\nüéâ SISTEMA H√çBRIDO CONCLU√çDO COM SUCESSO!")
    print(f"üìÅ Relat√≥rio: {ARQUIVO_SAIDA}")
    print(f"üéØ M√©todo usado: {metodo_usado}")
    print(f"üìä {len(df)} URLs processadas")

# ========================
# üöÄ Fun√ß√£o S√≠ncrona de Compatibilidade
# ========================
def main_sync():
    """üîÑ Wrapper s√≠ncrono para compatibilidade"""
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    try:
        asyncio.run(main_hibrido())
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Opera√ß√£o cancelada pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro cr√≠tico: {e}")
        import traceback
        traceback.print_exc()

# ========================
# üéØ EXECU√á√ÉO
# ========================
if __name__ == "__main__":
    print("üöÄ Iniciando Sistema SEO H√≠brido Enterprise...")
    
    if PLAYWRIGHT_AVAILABLE:
        print("‚úÖ Playwright dispon√≠vel - Modo h√≠brido ativo")
    else:
        print("‚ö†Ô∏è Playwright n√£o dispon√≠vel - Modo Requests apenas")
    
    main_sync()