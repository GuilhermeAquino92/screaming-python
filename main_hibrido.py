# main_hibrido.py - Sistema HÃ­brido Inteligente com Playwright Enterprise

import pandas as pd
import os
import asyncio
import sys
from urllib.parse import urlparse
from crawler import rastrear_profundo as crawler_requests
from status_checker import verificar_status_http
from metatags import extrair_metatags
from validador_headings import validar_headings
from http_inseguro import extrair_http_inseguros
from exporters.excel_manager import exportar_relatorio_completo
import warnings

# ğŸ†• Import Playwright (com fallback)
try:
    from crawler_playwright import rastrear_playwright_profundo
    PLAYWRIGHT_AVAILABLE = True
    print("âœ… Playwright Enterprise disponÃ­vel")
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸ Playwright nÃ£o instalado. Modo hÃ­brido desabilitado.")
    print("   Para instalar: pip install playwright && playwright install chromium")

warnings.filterwarnings("ignore")

# ========================
# ğŸ¯ CONFIGURAÃ‡Ã•ES HÃBRIDAS
# ========================
url_inicial = "https://gndisul.com.br/"
MAX_URLS = 3000
MAX_WORKERS = 20
MAX_DEPTH = 3
FORCAR_REINDEXACAO = False
PASTA_SAIDA = "output"
ARQUIVO_SAIDA = os.path.join(PASTA_SAIDA, "relatorio_seo_hibrido.xlsx")
os.makedirs(PASTA_SAIDA, exist_ok=True)

# ğŸ§  CONFIGURAÃ‡Ã•ES INTELIGENTES
MODO_CRAWLER = "AUTO"  # AUTO, REQUESTS, PLAYWRIGHT, HIBRIDO
LIMITE_JS_DETECTION = 50  # URLs para testar se precisa JS
FALLBACK_ATIVO = True  # Se Playwright falhar, usa Requests
USE_SELENIUM_SE_ERRO = True  # Selenium como Ãºltimo recurso

# ========================
# ğŸ¤– DetecÃ§Ã£o Inteligente de Site
# ========================
async def detectar_necessidade_js(url_inicial, limite=50):
    """ğŸ§  Testa amostra de URLs para ver se site precisa de JavaScript"""
    
    if not PLAYWRIGHT_AVAILABLE:
        return False, "Playwright nÃ£o disponÃ­vel"
    
    print(f"ğŸ” Testando se site precisa de JavaScript renderizaÃ§Ã£o...")
    print(f"ğŸ“Š Analisando atÃ© {limite} URLs de amostra...")
    
    try:
        # ğŸš€ Crawl pequeno com Playwright para teste
        amostra_resultados = await rastrear_playwright_profundo(
            url_inicial,
            max_urls=limite,
            max_depth=2,
            browser_pool_size=2
        )
        
        if not amostra_resultados:
            return False, "Nenhuma URL coletada para anÃ¡lise"
        
        # ğŸ“Š Analisa resultados
        total_urls = len(amostra_resultados)
        urls_com_js = len([r for r in amostra_resultados if r.get("needs_javascript", False)])
        percentual_js = (urls_com_js / total_urls * 100) if total_urls > 0 else 0
        
        # ğŸ” Coleta razÃµes detalhadas
        razoes_js = []
        for resultado in amostra_resultados:
            if resultado.get("needs_javascript", False):
                razao = resultado.get("js_detection_reason", "Desconhecido")
                if razao not in razoes_js:
                    razoes_js.append(razao)
        
        # ğŸ¯ DecisÃ£o inteligente
        precisa_js = percentual_js >= 30  # Se 30%+ das pÃ¡ginas precisam JS
        
        print(f"ğŸ“Š RESULTADO DA ANÃLISE:")
        print(f"   ğŸ”¢ URLs testadas: {total_urls}")
        print(f"   ğŸ¤– URLs que precisam JS: {urls_com_js} ({percentual_js:.1f}%)")
        print(f"   ğŸ“‹ Principais razÃµes: {', '.join(razoes_js[:3])}")
        print(f"   ğŸ¯ DecisÃ£o: {'PRECISA JavaScript' if precisa_js else 'Sites estÃ¡ticos OK'}")
        
        razao_final = f"{percentual_js:.1f}% das pÃ¡ginas precisam JS. RazÃµes: {', '.join(razoes_js[:2])}"
        
        return precisa_js, razao_final
        
    except Exception as e:
        print(f"âš ï¸ Erro na detecÃ§Ã£o JS: {e}")
        return True, f"Erro na detecÃ§Ã£o, usando Playwright por seguranÃ§a: {str(e)}"

# ========================
# ğŸš€ Sistema de Crawler HÃ­brido
# ========================
async def executar_crawler_hibrido():
    """ğŸ¯ Sistema inteligente que escolhe melhor crawler"""
    
    print(f"ğŸš€ Sistema HÃ­brido Inteligente Iniciado!")
    print(f"ğŸ¯ URL: {url_inicial}")
    print(f"ğŸ“Š ConfiguraÃ§Ã£o: {MAX_URLS} URLs, profundidade {MAX_DEPTH}")
    
    urls_com_dados = []
    metodo_usado = "INDEFINIDO"
    
    # ğŸ§  MODO AUTOMÃTICO - DetecÃ§Ã£o Inteligente
    if MODO_CRAWLER == "AUTO" and PLAYWRIGHT_AVAILABLE:
        print(f"\nğŸ§  MODO AUTO: Detectando melhor crawler...")
        
        try:
            precisa_js, razao = await detectar_necessidade_js(url_inicial, LIMITE_JS_DETECTION)
            
            if precisa_js:
                print(f"ğŸ¯ DECISÃƒO: Usar Playwright Enterprise (renderizaÃ§Ã£o JS necessÃ¡ria)")
                print(f"ğŸ“ RazÃ£o: {razao}")
                metodo_escolhido = "PLAYWRIGHT"
            else:
                print(f"ğŸ¯ DECISÃƒO: Usar Requests Otimizado (site estÃ¡tico detectado)")
                print(f"ğŸ“ RazÃ£o: {razao}")
                metodo_escolhido = "REQUESTS"
                
        except Exception as e:
            print(f"âš ï¸ Erro na detecÃ§Ã£o automÃ¡tica: {e}")
            print(f"ğŸ”„ Fallback: Usando Playwright por seguranÃ§a")
            metodo_escolhido = "PLAYWRIGHT"
    else:
        metodo_escolhido = MODO_CRAWLER
    
    # ğŸš€ EXECUÃ‡ÃƒO DO CRAWLER ESCOLHIDO
    if metodo_escolhido == "PLAYWRIGHT" and PLAYWRIGHT_AVAILABLE:
        try:
            print(f"\nğŸ­ Executando Playwright Enterprise...")
            urls_com_dados = await rastrear_playwright_profundo(
                url_inicial,
                max_urls=MAX_URLS,
                max_depth=MAX_DEPTH,
                forcar_reindexacao=FORCAR_REINDEXACAO,
                browser_pool_size=3
            )
            metodo_usado = "PLAYWRIGHT"
            print("âœ… Playwright Enterprise concluÃ­do com sucesso!")
            
        except Exception as e:
            print(f"âŒ Erro com Playwright: {e}")
            if FALLBACK_ATIVO:
                print(f"ğŸ”„ Executando fallback para Requests...")
                metodo_escolhido = "REQUESTS"
            else:
                raise e
    
    if metodo_escolhido == "REQUESTS":
        try:
            print(f"\nâš¡ Executando Requests Otimizado...")
            urls_com_dados = crawler_requests(
                url_inicial,
                max_urls=MAX_URLS,
                max_depth=MAX_DEPTH,
                max_workers=MAX_WORKERS,
                forcar_reindexacao=FORCAR_REINDEXACAO
            )
            metodo_usado = "REQUESTS"
            print("âœ… Requests Otimizado concluÃ­do com sucesso!")
            
        except Exception as e:
            print(f"âŒ Erro com Requests: {e}")
            if FALLBACK_ATIVO and USE_SELENIUM_SE_ERRO:
                print(f"ğŸ”„ Tentando Selenium como Ãºltimo recurso...")
                try:
                    from crawler_selenium import rastrear_selenium_profundo
                    urls_com_dados = rastrear_selenium_profundo(
                        url_inicial,
                        max_urls=MAX_URLS,
                        forcar_reindexacao=FORCAR_REINDEXACAO
                    )
                    metodo_usado = "SELENIUM"
                    print("âœ… Selenium concluÃ­do com sucesso!")
                except Exception as selenium_error:
                    print(f"âŒ Erro tambÃ©m com Selenium: {selenium_error}")
                    urls_com_dados = []
                    metodo_usado = "ERRO"
            else:
                urls_com_dados = []
                metodo_usado = "ERRO"
    
    # ğŸ¯ MODO HÃBRIDO - Usa ambos para comparaÃ§Ã£o
    if metodo_escolhido == "HIBRIDO" and PLAYWRIGHT_AVAILABLE:
        print(f"\nğŸ”„ MODO HÃBRIDO: Executando Requests + Playwright...")
        
        try:
            # Executa ambos em paralelo (cuidado com recursos)
            print(f"âš¡ Executando Requests primeiro...")
            urls_requests = crawler_requests(
                url_inicial,
                max_urls=MAX_URLS//2,  # Divide URLs
                max_depth=MAX_DEPTH,
                max_workers=MAX_WORKERS//2,
                forcar_reindexacao=FORCAR_REINDEXACAO
            )
            
            print(f"ğŸ­ Executando Playwright para comparaÃ§Ã£o...")
            urls_playwright = await rastrear_playwright_profundo(
                url_inicial,
                max_urls=MAX_URLS//2,
                max_depth=MAX_DEPTH,
                forcar_reindexacao=FORCAR_REINDEXACAO,
                browser_pool_size=2
            )
            
            # Combina resultados (prioriza Playwright para dados SEO)
            urls_dict = {}
            
            # Adiciona dados do Requests
            for item in urls_requests:
                url = item.get('url')
                if url:
                    urls_dict[url] = item
            
            # Atualiza/sobrescreve com dados do Playwright (mais precisos)
            for item in urls_playwright:
                url = item.get('url')
                if url:
                    if url in urls_dict:
                        # MantÃ©m dados do Requests, atualiza com dados SEO do Playwright
                        urls_dict[url].update({
                            k: v for k, v in item.items() 
                            if k in ['title', 'description', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
                                   'needs_javascript', 'structured_data_count']
                        })
                    else:
                        urls_dict[url] = item
            
            urls_com_dados = list(urls_dict.values())
            metodo_usado = "HÃBRIDO"
            print(f"âœ… Modo HÃ­brido concluÃ­do! Combinados {len(urls_dict)} URLs Ãºnicos")
            
        except Exception as e:
            print(f"âŒ Erro no modo hÃ­brido: {e}")
            print(f"ğŸ”„ Fallback para Requests apenas...")
            urls_com_dados = crawler_requests(
                url_inicial,
                max_urls=MAX_URLS,
                max_depth=MAX_DEPTH,
                max_workers=MAX_WORKERS,
                forcar_reindexacao=FORCAR_REINDEXACAO
            )
            metodo_usado = "REQUESTS_FALLBACK"
    
    return urls_com_dados, metodo_usado

# ========================
# ğŸ” Processamento de Dados HÃ­brido
# ========================
def processar_dados_hibridos(df, metodo_usado):
    """ğŸ¯ Processa dados considerando o mÃ©todo usado"""
    
    print(f"\nğŸ“Š Processando dados coletados via {metodo_usado}...")
    
    # âœ… COMPATIBILIDADE - Normaliza nomes de colunas
    colunas_mapping = {
        'status_code': 'status_code_http',  # Padroniza para auditoria
    }
    
    for old_col, new_col in colunas_mapping.items():
        if old_col in df.columns and new_col not in df.columns:
            df = df.rename(columns={old_col: new_col})
    
    # ğŸ¯ DADOS ESPECÃFICOS DO PLAYWRIGHT
    if metodo_usado in ["PLAYWRIGHT", "HÃBRIDO"]:
        print(f"ğŸ­ Dados Playwright detectados - processamento avanÃ§ado...")
        
        # EstatÃ­sticas de JavaScript
        if 'needs_javascript' in df.columns:
            total_js = len(df[df['needs_javascript'] == True])
            total_estatico = len(df[df['needs_javascript'] == False])
            print(f"   ğŸ¤– Sites com JS: {total_js}")
            print(f"   ğŸ“„ Sites estÃ¡ticos: {total_estatico}")
        
        # Structured Data
        if 'structured_data_count' in df.columns:
            com_structured = len(df[df['structured_data_count'] > 0])
            print(f"   ğŸ“Š PÃ¡ginas com dados estruturados: {com_structured}")
        
        # Console Errors
        if 'console_errors' in df.columns:
            com_erros = len(df[df['console_errors'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)])
            print(f"   ğŸš¨ PÃ¡ginas com erros JavaScript: {com_erros}")
    
    # ğŸ” DADOS ESPECÃFICOS DO REQUESTS
    elif metodo_usado == "REQUESTS":
        print(f"âš¡ Dados Requests detectados - processamento otimizado...")
        
        if 'response_time' in df.columns:
            tempo_medio = df['response_time'].mean()
            print(f"   â±ï¸ Tempo mÃ©dio de resposta: {tempo_medio:.2f}ms")
    
    return df

# ========================
# ğŸ¯ MAIN ASSÃNCRONO HÃBRIDO
# ========================
async def main_hibrido():
    """ğŸš€ FunÃ§Ã£o principal hÃ­brida assÃ­ncrona"""
    
    print("=" * 60)
    print("ğŸš€ SISTEMA SEO HÃBRIDO ENTERPRISE")
    print("=" * 60)
    
    # ğŸ” FASE 1: Crawling HÃ­brido Inteligente
    print(f"\nğŸ“¡ FASE 1: CRAWLING HÃBRIDO INTELIGENTE")
    urls_com_dados, metodo_usado = await executar_crawler_hibrido()
    
    if not urls_com_dados:
        print("âŒ ERRO CRÃTICO: Nenhuma URL coletada!")
        sys.exit(1)
    
    df = pd.DataFrame(urls_com_dados)
    print(f"ğŸ“Š DataFrame criado: {len(df)} URLs, colunas: {list(df.columns)}")
    
    # ğŸ¯ FASE 2: Processamento de Dados
    print(f"\nğŸ”§ FASE 2: PROCESSAMENTO DE DADOS")
    df = processar_dados_hibridos(df, metodo_usado)
    
    # ğŸš« FASE 3: Filtros de Qualidade
    print(f"\nğŸ¯ FASE 3: FILTROS DE QUALIDADE")
    EXTENSOES_INVALIDAS = [
        '.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.bmp', '.ico',
        '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar', '.7z',
        '.js', '.css', '.mp3', '.mp4', '.avi', '.mov'
    ]
    
    urls_antes = len(df)
    df = df[~df['url'].str.lower().str.contains('|'.join([f'{ext}$' for ext in EXTENSOES_INVALIDAS]), na=False)]
    urls_depois = len(df)
    print(f"ğŸ“ Filtro HTML aplicado: {urls_antes} â†’ {urls_depois} URLs (-{urls_antes-urls_depois} arquivos)")
    
    # ğŸ” FASE 4: VerificaÃ§Ãµes Adicionais (apenas se necessÃ¡rio)
    print(f"\nğŸ” FASE 4: VERIFICAÃ‡Ã•ES ADICIONAIS")
    
    # Status HTTP - sÃ³ se Playwright nÃ£o coletou ou dados inconsistentes
    if metodo_usado != "PLAYWRIGHT" or 'status_code_http' not in df.columns:
        print(f"ğŸ” Verificando status HTTP independente...")
        urls_http = df['url'].dropna().unique().tolist()
        df_status = pd.DataFrame(verificar_status_http(urls_http, max_threads=50))
        df = df.merge(df_status, on='url', how='left', suffixes=('', '_check'))
        
        # Usa verificaÃ§Ã£o independente se nÃ£o tinha dados
        if 'status_code_http' not in df.columns and 'status_code_http_check' in df.columns:
            df['status_code_http'] = df['status_code_http_check']
    else:
        print(f"âœ… Status HTTP jÃ¡ coletado via {metodo_usado}")
    
    # Metatags - sÃ³ se Playwright nÃ£o coletou dados completos
    if metodo_usado != "PLAYWRIGHT" or 'title' not in df.columns:
        print(f"ğŸ” Extraindo metatags independente...")
        urls_meta = df['url'].dropna().unique().tolist()
        df_meta = pd.DataFrame(extrair_metatags(urls_meta, max_threads=50))
        df = df.merge(df_meta, on='url', how='left', suffixes=('_pw', ''))
        
        # Prioriza dados do Playwright se existirem
        for col in ['title', 'description']:
            if f'{col}_pw' in df.columns:
                df[col] = df[f'{col}_pw'].fillna(df[col])
                df = df.drop(f'{col}_pw', axis=1)
    else:
        print(f"âœ… Metatags jÃ¡ coletadas via {metodo_usado}")
    
    # Headings - SEMPRE executar para anÃ¡lise CSS detalhada
    print(f"ğŸ§  Validando headings com detecÃ§Ã£o CSS...")
    urls_head = df['url'].dropna().unique().tolist()
    df_head = pd.DataFrame(validar_headings(urls_head, max_threads=30))
    df = df.merge(df_head, on='url', how='left', suffixes=('_pw', '_css'))
    
    # Combina dados de heading quando disponÃ­vel
    for col in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        if f'{col}_pw' in df.columns and f'{col}_css' in df.columns:
            # Prioriza dados do CSS analyzer (mais preciso para anÃ¡lise)
            df[col] = df[f'{col}_css'].fillna(df[f'{col}_pw'])
        elif f'{col}_css' in df.columns:
            df[col] = df[f'{col}_css']
        elif f'{col}_pw' in df.columns:
            df[col] = df[f'{col}_pw']
    
    # Limpa colunas temporÃ¡rias
    colunas_temp = [col for col in df.columns if col.endswith('_pw') or col.endswith('_css')]
    df = df.drop(colunas_temp, axis=1, errors='ignore')
    
    # HTTP Inseguro
    print(f"ğŸš¨ Analisando HTTP inseguro...")
    urls_http_inseguro = df['url'].dropna().unique().tolist()
    df_http = pd.DataFrame(extrair_http_inseguros(urls_http_inseguro, max_threads=40))
    
    # ğŸ“‹ FASE 5: Auditorias SEO
    print(f"\nğŸ“‹ FASE 5: AUDITORIAS SEO")
    
    # Garante colunas necessÃ¡rias
    for col in ['title', 'description']:
        if col not in df.columns:
            df[col] = ''
    
    # Filtro para auditoria (remove paginaÃ§Ã£o)
    df_filtrado = df[~df["url"].str.contains(r"\?page=\d+", na=False)].copy()
    
    # Auditorias originais
    df_title_ausente = df_filtrado[df_filtrado["title"].str.strip() == ""].copy()
    df_description_ausente = df_filtrado[df_filtrado["description"].str.strip() == ""].copy()
    
    # Duplicados
    title_count = df_filtrado["title"].dropna().str.strip()
    desc_count = df_filtrado["description"].dropna().str.strip()
    title_count = title_count[title_count != ""]
    desc_count = desc_count[desc_count != ""]
    
    titles_duplicados = title_count.value_counts()
    descs_duplicados = desc_count.value_counts()
    titles_duplicados = titles_duplicados[titles_duplicados > 1].index.tolist()
    descs_duplicados = descs_duplicados[descs_duplicados > 1].index.tolist()
    
    df_title_duplicado = df_filtrado[df_filtrado["title"].isin(titles_duplicados)].copy()
    df_description_duplicado = df_filtrado[df_filtrado["description"].isin(descs_duplicados)].copy()
    
    # Erros HTTP
    status_col = 'status_code_http' if 'status_code_http' in df.columns else 'status_code'
    if status_col in df.columns:
        df_errors = df[df[status_col].astype(str).str.startswith(('3', '4', '5'))].copy()
        if not df_errors.empty:
            df_errors["tipo_erro"] = df_errors[status_col].astype(str).str[0] + "xx"
    else:
        df_errors = pd.DataFrame()
    
    # ğŸ“Š FASE 6: EstatÃ­sticas Finais
    print(f"\nğŸ“Š FASE 6: ESTATÃSTICAS FINAIS")
    print(f"ğŸ¯ MÃ©todo usado: {metodo_usado}")
    print(f"ğŸ“ Total de URLs: {len(df)}")
    
    if status_col in df.columns:
        status_200 = len(df[df[status_col] == 200])
        status_3xx = len(df[df[status_col].astype(str).str.startswith('3')])
        status_4xx = len(df[df[status_col].astype(str).str.startswith('4')])
        status_5xx = len(df[df[status_col].astype(str).str.startswith('5')])
        
        print(f"âœ… Status 200: {status_200}")
        print(f"ğŸ”„ Redirecionamentos: {status_3xx}")
        print(f"âŒ Erros 4xx: {status_4xx}")
        print(f"ğŸš¨ Erros 5xx: {status_5xx}")
    
    print(f"ğŸ“‹ Title ausente: {len(df_title_ausente)}")
    print(f"ğŸ“‹ Description ausente: {len(df_description_ausente)}")
    print(f"ğŸ”„ Title duplicado: {len(df_title_duplicado)}")
    print(f"ğŸ”„ Description duplicado: {len(df_description_duplicado)}")
    
    # EstatÃ­sticas de headings
    if 'headings_vazios_count' in df.columns:
        total_vazios = df['headings_vazios_count'].sum()
        urls_vazios = len(df[df['headings_vazios_count'] > 0])
        print(f"ğŸ•³ï¸ Headings vazios: {total_vazios} em {urls_vazios} URLs")
    
    if 'headings_ocultos_count' in df.columns:
        total_ocultos = df['headings_ocultos_count'].sum()
        urls_ocultos = len(df[df['headings_ocultos_count'] > 0])
        print(f"ğŸ¨ Headings ocultos CSS: {total_ocultos} em {urls_ocultos} URLs")
    
    # ğŸ“Š FASE 7: ExportaÃ§Ã£o
    print(f"\nğŸš€ FASE 7: EXPORTAÃ‡ÃƒO DO RELATÃ“RIO")
    
    auditorias = {
        "df_title_ausente": df_title_ausente,
        "df_description_ausente": df_description_ausente,
        "df_title_duplicado": df_title_duplicado,
        "df_description_duplicado": df_description_duplicado,
        "df_errors": df_errors
    }
    
    # Adiciona metadados do crawler usado
    df['crawler_method'] = metodo_usado
    df['crawler_version'] = 'HÃ­brido v2.0'
    
    exportar_relatorio_completo(df, df_http, auditorias, ARQUIVO_SAIDA)
    
    print(f"\nğŸ‰ SISTEMA HÃBRIDO CONCLUÃDO COM SUCESSO!")
    print(f"ğŸ“ RelatÃ³rio: {ARQUIVO_SAIDA}")
    print(f"ğŸ¯ MÃ©todo usado: {metodo_usado}")
    print(f"ğŸ“Š {len(df)} URLs processadas")

# ========================
# ğŸš€ FunÃ§Ã£o SÃ­ncrona de Compatibilidade
# ========================
def main_sync():
    """ğŸ”„ Wrapper sÃ­ncrono para compatibilidade"""
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    try:
        asyncio.run(main_hibrido())
    except KeyboardInterrupt:
        print("\nâš ï¸ OperaÃ§Ã£o cancelada pelo usuÃ¡rio")
    except Exception as e:
        print(f"\nâŒ Erro crÃ­tico: {e}")
        import traceback
        traceback.print_exc()

# ========================
# ğŸ¯ EXECUÃ‡ÃƒO
# ========================
if __name__ == "__main__":
    print("ğŸš€ Iniciando Sistema SEO HÃ­brido Enterprise...")
    
    if PLAYWRIGHT_AVAILABLE:
        print("âœ… Playwright disponÃ­vel - Modo hÃ­brido ativo")
    else:
        print("âš ï¸ Playwright nÃ£o disponÃ­vel - Modo Requests apenas")
    
    main_sync()