# main_hibrido.py - Pipeline Orquestrador LEAN + URLManager SEO

import pandas as pd
import os
import asyncio
import sys
import datetime
from urllib.parse import urlparse

# Imports dos crawlers
from crawler import rastrear_profundo as crawler_requests
from status_checker import verificar_status_http
from metatags import extrair_metatags
from validador_headings import validar_headings
from http_inseguro import extrair_http_inseguros

# Excel Manager
try:
    from exporters.excel_manager import exportar_relatorio_completo
    EXCEL_MANAGER_AVAILABLE = True
    print("‚úÖ Excel Manager especializado dispon√≠vel")
except ImportError:
    print("‚ö†Ô∏è Excel Manager n√£o dispon√≠vel - usando vers√£o b√°sica")
    EXCEL_MANAGER_AVAILABLE = False
    
    def exportar_relatorio_completo(df, df_http, auditorias, output_path):
        """üìä Vers√£o b√°sica de exporta√ß√£o"""
        try:
            if not os.path.isabs(output_path):
                output_path = os.path.join(os.getcwd(), os.path.basename(output_path))
            
            with pd.ExcelWriter(output_path, engine="xlsxwriter") as writer:
                df.to_excel(writer, sheet_name='Dados_Principais', index=False)
                
                for nome, df_aud in auditorias.items():
                    if df_aud is not None and not df_aud.empty:
                        nome_aba = nome.replace('df_', '').title()
                        df_aud.to_excel(writer, sheet_name=nome_aba, index=False)
                
                if not df_http.empty:
                    df_http.to_excel(writer, sheet_name='HTTP_Inseguro', index=False)
            
            print(f"‚úÖ Arquivo Excel b√°sico criado: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"‚ùå Erro na exporta√ß√£o b√°sica: {e}")
            csv_path = output_path.replace('.xlsx', '.csv')
            df.to_csv(csv_path, index=False, encoding='utf-8')
            print(f"üîÑ Dados salvos como CSV: {csv_path}")
            return csv_path

# Playwright LEAN
try:
    from crawler_playwright import rastrear_playwright_profundo
    PLAYWRIGHT_AVAILABLE = True
    print("‚úÖ Playwright LEAN dispon√≠vel")
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("‚ö†Ô∏è Playwright n√£o dispon√≠vel")

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

# ========================
# üéØ CONFIGURA√á√ÉO GLOBAL PIPELINE
# ========================

URL_BASE = "https://ccgsaude.com.br"
MAX_URLS = 1000
MAX_DEPTH = 3

def gerar_nome_arquivo_seguro(url_base):
    """üîß Gera nome de arquivo seguro"""
    import re
    nome_limpo = url_base.replace('https://', '').replace('http://', '')
    nome_limpo = re.sub(r'[<>:"/\\|?*]', '_', nome_limpo)
    nome_limpo = nome_limpo.replace('.', '_').replace('/', '_')
    return f"relatorio_seo_pipeline_{nome_limpo}.xlsx"

ARQUIVO_SAIDA = gerar_nome_arquivo_seguro(URL_BASE)

# ========================
# üß† DETECTOR DE NECESSIDADE JS SIMPLES
# ========================

async def detectar_necessidade_js_simples(url: str) -> tuple[bool, str]:
    """üß† Detec√ß√£o simples e eficaz de necessidade de JS"""
    
    try:
        import requests
        
        # Testa vers√£o sem JS
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10, verify=False)
        html_lower = response.text.lower()
        
        # Detectores simples
        js_indicators = 0
        reasons = []
        
        # Framework detection
        if any(fw in html_lower for fw in ['react', 'vue', 'angular', 'next.js']):
            js_indicators += 2
            reasons.append("Framework JS detectado")
        
        # SPA patterns
        if any(pattern in html_lower for pattern in ['data-reactroot', 'ng-app', 'v-app']):
            js_indicators += 2
            reasons.append("SPA pattern detectado")
        
        # Dynamic content indicators
        if any(word in html_lower for word in ['loading', 'carregando', 'spinner']):
            js_indicators += 1
            reasons.append("Indicadores de loading din√¢mico")
        
        # API calls
        if any(api in html_lower for api in ['fetch(', 'axios', '$.ajax', 'api/']):
            js_indicators += 1
            reasons.append("Chamadas de API detectadas")
        
        needs_js = js_indicators >= 2
        reason = " | ".join(reasons) if reasons else "Site aparenta ser est√°tico"
        
        return needs_js, reason
        
    except Exception as e:
        return True, f"Erro na detec√ß√£o: {str(e)} - usando Playwright por seguran√ßa"

# ========================
# üöÄ PIPELINE PRINCIPAL H√çBRIDO
# ========================

async def executar_pipeline_hibrido():
    """üöÄ Pipeline h√≠brido inteligente e simples"""
    
    print("üöÄ PIPELINE SEO H√çBRIDO LEAN")
    print("="*50)
    
    urls_com_dados = []
    metodo_usado = "INDEFINIDO"
    
    # üß† DETEC√á√ÉO AUTOM√ÅTICA
    if PLAYWRIGHT_AVAILABLE:
        print(f"üß† Detectando necessidade de JS para: {URL_BASE}")
        
        try:
            precisa_js, razao = await detectar_necessidade_js_simples(URL_BASE)
            
            print(f"üìã Resultado: {'Playwright' if precisa_js else 'Requests'}")
            print(f"üìã Raz√£o: {razao}")
            
            if precisa_js:
                print(f"\nüé≠ Executando Playwright LEAN...")
                urls_com_dados = await rastrear_playwright_profundo(
                    URL_BASE,
                    max_urls=MAX_URLS,
                    max_depth=MAX_DEPTH,
                    forcar_reindexacao=False
                )
                metodo_usado = "PLAYWRIGHT_LEAN"
            else:
                print(f"\n‚ö° Executando Requests Otimizado...")
                urls_com_dados = crawler_requests(
                    URL_BASE,
                    max_urls=MAX_URLS,
                    max_depth=MAX_DEPTH
                )
                metodo_usado = "REQUESTS"
                
        except Exception as e:
            print(f"‚ùå Erro na detec√ß√£o/execu√ß√£o: {e}")
            print(f"üîÑ Fallback para Requests...")
            metodo_usado = "REQUESTS"
    
    # Fallback para Requests se Playwright n√£o dispon√≠vel
    if not urls_com_dados:
        print(f"\n‚ö° Executando Requests (fallback)...")
        try:
            urls_com_dados = crawler_requests(
                URL_BASE,
                max_urls=MAX_URLS,
                max_depth=MAX_DEPTH
            )
            metodo_usado = "REQUESTS"
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico no Requests: {e}")
            return [], "ERRO"
    
    print(f"‚úÖ Crawling conclu√≠do: {len(urls_com_dados)} URLs")
    return urls_com_dados, metodo_usado

# ========================
# üìä PIPELINE DE AN√ÅLISE
# ========================

def analisar_distribuicao_tipos_url_simples(df):
    """üìä An√°lise simples de tipos de URL"""
    
    if df.empty:
        return {}
    
    tipos_url = {}
    
    for _, row in df.iterrows():
        url = row.get('url', '')
        if not url:
            continue
        
        path = urlparse(url).path.lower()
        
        # Classifica√ß√£o simples
        if path in ['', '/']:
            tipo = 'homepage'
        elif any(termo in path for termo in ['/blog', '/post', '/artigo', '/news']):
            tipo = 'conteudo'
        elif any(termo in path for termo in ['/produto', '/product']):
            tipo = 'produto'
        elif any(termo in path for termo in ['/categoria', '/category']):
            tipo = 'categoria'
        elif any(termo in path for termo in ['/sobre', '/contato', '/servicos']):
            tipo = 'institucional'
        else:
            tipo = 'outros'
        
        tipos_url[tipo] = tipos_url.get(tipo, 0) + 1
    
    # Log da distribui√ß√£o
    total = sum(tipos_url.values())
    print(f"\nüìä DISTRIBUI√á√ÉO DE TIPOS ({total} URLs):")
    for tipo, count in sorted(tipos_url.items(), key=lambda x: x[1], reverse=True):
        percent = (count / total) * 100 if total > 0 else 0
        icones = {'homepage': 'üè†', 'conteudo': 'üìù', 'produto': 'üõí', 'categoria': 'üìÅ', 'institucional': 'üè¢', 'outros': 'üìÑ'}
        icone = icones.get(tipo, 'üìÑ')
        print(f"   {icone} {tipo.capitalize()}: {count} ({percent:.1f}%)")
    
    return tipos_url

# ========================
# üî• PATCH H√çBRIDO (ORIGINAL)
# ========================

def aplicar_patch_headings_hibrido(excel_path: str) -> str:
    """üî• Aplica patch h√≠brido para headings vazios"""
    
    try:
        print(f"\nüî• APLICANDO PATCH H√çBRIDO HEADINGS...")
        print(f"üìÅ Arquivo: {excel_path}")
        
        # Import do revalidador
        from revalidador_headings_hibrido import RevalidadorHeadingsHibridoOtimizado as RevalidadorHeadingsHibrido        
        # Aplica patch
        revalidador = RevalidadorHeadingsHibrido()
        excel_corrigido = revalidador.revalidar_excel_completo(
            excel_path, 
            excel_path.replace('.xlsx', '_HIBRIDO.xlsx')
        )
        
        print(f"‚úÖ PATCH H√çBRIDO APLICADO!")
        print(f"üìÅ Arquivo corrigido: {excel_corrigido}")
        
        return excel_corrigido
        
    except Exception as e:
        print(f"‚ùå Erro no patch h√≠brido: {e}")
        print(f"üìã Mantendo arquivo original: {excel_path}")
        return excel_path

# ========================
# üî• PATCH CIR√öRGICO COMPLETO
# ========================

def aplicar_patch_cirurgico_completo(excel_path: str) -> str:
    """üî• Aplica patch CIR√öRGICO completo AUTOM√ÅTICO: headings + titles"""
    
    try:
        print(f"üî• PATCH CIR√öRGICO AUTOM√ÅTICO INICIADO")
        print(f"üìÅ Arquivo base: {excel_path}")
        
        # 1. PATCH HEADINGS CIR√öRGICO (AUTOM√ÅTICO)
        print(f"\nüéØ 1/2 - HEADINGS VAZIOS CIR√öRGICO (AUTOM√ÅTICO):")
        from revalidador_headings_hibrido import revalidar_headings_excel_cirurgico
        
        excel_com_headings = revalidar_headings_excel_cirurgico(
            excel_path, 
            excel_path.replace('.xlsx', '_TEMP_HEADINGS.xlsx')
        )
        
        # 2. PATCH TITLES CIR√öRGICO (AUTOM√ÅTICO) - COM FALLBACK
        print(f"\nüéØ 2/2 - TITLES AUSENTES CIR√öRGICO (AUTOM√ÅTICO):")
        try:
            from revalidador_title_cirurgico import revalidar_titles_excel_cirurgico
            
            excel_final = revalidar_titles_excel_cirurgico(
                excel_com_headings,
                excel_path.replace('.xlsx', '_CIRURGICO_COMPLETO.xlsx')
            )
            print(f"‚úÖ Patch de titles aplicado com sucesso!")
            
        except ImportError as e:
            print(f"‚ö†Ô∏è M√≥dulo de titles n√£o dispon√≠vel: {e}")
            print(f"üìã Continuando apenas com patch de headings...")
            excel_final = excel_com_headings
        
        # 3. LIMPA ARQUIVO TEMPOR√ÅRIO
        try:
            if os.path.exists(excel_com_headings) and 'TEMP' in excel_com_headings:
                os.remove(excel_com_headings)
                print(f"üóëÔ∏è Arquivo tempor√°rio removido")
        except:
            pass
        
        print(f"\n‚úÖ PATCH CIR√öRGICO AUTOM√ÅTICO CONCLU√çDO!")
        print(f"üìÅ Arquivo final: {excel_final}")
        print(f"üéØ Cont√©m: Headings Vazios (+ Titles se dispon√≠vel)")
        
        return excel_final
        
    except Exception as e:
        print(f"‚ùå Erro no patch cir√∫rgico: {e}")
        print(f"üìã Mantendo arquivo original: {excel_path}")
        return excel_path

# ========================
# üéØ MAIN PIPELINE
# ========================

async def main_pipeline():
    """üéØ Pipeline principal orquestrador"""
    
    print("üéØ SISTEMA SEO PIPELINE LEAN")
    print("="*50)
    print(f"üìã Configura√ß√£o:")
    print(f"   URL: {URL_BASE}")
    print(f"   Max URLs: {MAX_URLS}")
    print(f"   Max Depth: {MAX_DEPTH}")
    print(f"   Arquivo: {ARQUIVO_SAIDA}")
    print(f"   Playwright: {'‚úÖ' if PLAYWRIGHT_AVAILABLE else '‚ùå'}")
    print(f"   Excel Manager: {'‚úÖ' if EXCEL_MANAGER_AVAILABLE else '‚ùå'}")
    
    # üöÄ FASE 1: CRAWLING H√çBRIDO
    print(f"\nüöÄ FASE 1: CRAWLING H√çBRIDO")
    urls_com_dados, metodo_usado = await executar_pipeline_hibrido()
    
    if not urls_com_dados:
        print("‚ùå ERRO: Nenhuma URL coletada!")
        sys.exit(1)
    
    df = pd.DataFrame(urls_com_dados)
    print(f"üìä DataFrame: {len(df)} URLs, {len(df.columns)} colunas")
    
    # üìä FASE 2: AN√ÅLISE DE DADOS
    print(f"\nüìä FASE 2: AN√ÅLISE DE DADOS")
    tipos_url = analisar_distribuicao_tipos_url_simples(df)
    
    # üîç FASE 3: VERIFICA√á√ïES COMPLEMENTARES
    print(f"\nüîç FASE 3: VERIFICA√á√ïES COMPLEMENTARES")
    
    # Status HTTP se necess√°rio
    if metodo_usado != "PLAYWRIGHT_LEAN" or 'status_code_http' not in df.columns:
        print(f"üîç Verificando status HTTP...")
        urls_http = df['url'].dropna().unique().tolist()
        df_status = pd.DataFrame(verificar_status_http(urls_http, max_threads=50))
        df = df.merge(df_status, on='url', how='left', suffixes=('', '_check'))
        
        if 'status_code_http' not in df.columns and 'status_code_http_check' in df.columns:
            df['status_code_http'] = df['status_code_http_check']
    
    # Metatags se necess√°rio
    if metodo_usado != "PLAYWRIGHT_LEAN" or 'title' not in df.columns:
        print(f"üìã Extraindo metatags...")
        urls_meta = df['url'].dropna().unique().tolist()
        df_meta = pd.DataFrame(extrair_metatags(urls_meta, max_threads=50))
        df = df.merge(df_meta, on='url', how='left', suffixes=('_pw', ''))
        
        for col in ['title', 'description']:
            if f'{col}_pw' in df.columns:
                df[col] = df[f'{col}_pw'].fillna(df[col])
                df = df.drop(f'{col}_pw', axis=1)
    
    # Headings - REMOVIDO (patch cir√∫rgico faz melhor depois)
    print(f"üè∑Ô∏è Headings ser√£o validados no patch cir√∫rgico (mais preciso)...")
    
    # HTTP Inseguro
    print(f"üîí Analisando HTTP inseguro...")
    urls_http_inseguro = df['url'].dropna().unique().tolist()
    df_http = pd.DataFrame(extrair_http_inseguros(urls_http_inseguro, max_threads=40))
    
    # üìã FASE 4: AUDITORIAS SEO
    print(f"\nüìã FASE 4: AUDITORIAS SEO")
    
    # Garante colunas necess√°rias
    for col in ['title', 'description']:
        if col not in df.columns:
            df[col] = ''
    
    # Filtro para auditoria
    df_filtrado = df[~df["url"].str.contains(r"\?page=\d+", na=False)].copy()
    
    # Auditorias b√°sicas
    df_title_ausente = df_filtrado[df_filtrado["title"].str.strip() == ""].copy()
    df_description_ausente = df_filtrado[df_filtrado["description"].str.strip() == ""].copy()
    
    # Duplicados
    title_count = df_filtrado["title"].dropna().str.strip()
    desc_count = df_filtrado["description"].dropna().str.strip()
    title_count = title_count[title_count != ""]
    desc_count = desc_count[desc_count != ""]
    
    titles_duplicados = title_count.value_counts()
    descs_duplicados = desc_count.value_counts()
    titles_duplicados = titles_duplicados[titles_duplicados > 1].index.tolist()
    descs_duplicados = descs_duplicados[descs_duplicados > 1].index.tolist()
    
    df_title_duplicado = df_filtrado[df_filtrado["title"].isin(titles_duplicados)].copy()
    df_description_duplicado = df_filtrado[df_filtrado["description"].isin(descs_duplicados)].copy()
    
    # Erros HTTP
    status_col = 'status_code_http' if 'status_code_http' in df.columns else 'status_code'
    if status_col in df.columns:
        df_errors = df[df[status_col].astype(str).str.startswith(('3', '4', '5'))].copy()
        if not df_errors.empty:
            df_errors["tipo_erro"] = df_errors[status_col].astype(str).str[0] + "xx"
    else:
        df_errors = pd.DataFrame()
    
    # üìä FASE 5: ESTAT√çSTICAS FINAIS
    print(f"\nüìä FASE 5: ESTAT√çSTICAS FINAIS")
    print(f"üéØ M√©todo usado: {metodo_usado}")
    print(f"üìù Total de URLs: {len(df)}")
    
    if status_col in df.columns:
        status_200 = len(df[df[status_col] == 200])
        status_3xx = len(df[df[status_col].astype(str).str.startswith('3')])
        status_4xx = len(df[df[status_col].astype(str).str.startswith('4')])
        status_5xx = len(df[df[status_col].astype(str).str.startswith('5')])
        
        print(f"‚úÖ Status 200: {status_200}")
        print(f"üîÑ Redirecionamentos: {status_3xx}")
        print(f"‚ùå Erros 4xx: {status_4xx}")
        print(f"üö® Erros 5xx: {status_5xx}")
    
    print(f"üìã Title ausente: {len(df_title_ausente)}")
    print(f"üìã Description ausente: {len(df_description_ausente)}")
    print(f"üîÑ Title duplicado: {len(df_title_duplicado)}")
    print(f"üîÑ Description duplicado: {len(df_description_duplicado)}")
    
    # üì§ FASE 6: EXPORTA√á√ÉO
    print(f"\nüì§ FASE 6: EXPORTA√á√ÉO")
    
    auditorias = {
        "df_title_ausente": df_title_ausente,
        "df_description_ausente": df_description_ausente,
        "df_title_duplicado": df_title_duplicado,
        "df_description_duplicado": df_description_duplicado,
        "df_errors": df_errors
    }
    
    # Metadados
    df['crawler_method'] = metodo_usado
    df['crawler_version'] = 'pipeline_lean_v1.0'
    df['analise_timestamp'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Exporta√ß√£o segura
    try:
        if not os.path.isabs(ARQUIVO_SAIDA):
            arquivo_final = os.path.join(os.getcwd(), os.path.basename(ARQUIVO_SAIDA))
        else:
            arquivo_final = ARQUIVO_SAIDA
        
        # üì§ EXPORTA√á√ÉO INICIAL
        excel_inicial = exportar_relatorio_completo(df, df_http, auditorias, ARQUIVO_SAIDA)
        print(f"‚úÖ Relat√≥rio inicial exportado: {excel_inicial}")

        # ‚úÖ EXPORTA√á√ÉO COM AN√ÅLISE COMPLETA (usa excel_manager existente)
        print(f"\nüìä EXPORTANDO RELAT√ìRIO COMPLETO...")
        print(f"   ‚úÖ Usa estrutura existente do excel_manager.py")
        print(f"   üìã Abas: Headings_Vazios, Estrutura_Headings, H1_H2_Problemas")
        print(f"   üéØ Sistema integrado - sem duplica√ß√£o!")

        arquivo_final = excel_inicial
        
    except Exception as e:
        print(f"‚ùå Erro na exporta√ß√£o: {e}")
        
        # Fallback CSV
        arquivo_csv = ARQUIVO_SAIDA.replace('.xlsx', '.csv')
        arquivo_csv = os.path.join(os.getcwd(), os.path.basename(arquivo_csv))
        df.to_csv(arquivo_csv, index=False, encoding='utf-8')
        print(f"üîÑ Dados salvos como CSV: {arquivo_csv}")
        arquivo_final = arquivo_csv
    
    # üéâ FASE 7: RELAT√ìRIO FINAL
    print(f"\nüéâ PIPELINE CONCLU√çDO COM SUCESSO!")
    print("="*50)
    print(f"üìÅ Arquivo: {arquivo_final}")
    print(f"üéØ M√©todo: {metodo_usado}")
    print(f"üìä URLs: {len(df)}")
    print(f"üìà Tipos identificados: {len(tipos_url)}")
    
    # Recomenda√ß√µes simples
    print(f"\nüí° RECOMENDA√á√ïES:")
    if len(df_title_ausente) > 0:
        print(f"   üìù {len(df_title_ausente)} p√°ginas sem title precisam de aten√ß√£o")
    
    if len(df_errors) > 0:
        print(f"   üö® {len(df_errors)} URLs com erros HTTP precisam ser corrigidas")
    
    titles_ok = len(df) - len(df_title_ausente)
    title_rate = (titles_ok / len(df)) * 100 if len(df) > 0 else 0
    
    if title_rate < 90:
        print(f"   ‚ö†Ô∏è Taxa de captura de titles baixa ({title_rate:.1f}%) - considere usar Playwright")
    else:
        print(f"   ‚úÖ Taxa de captura de titles boa ({title_rate:.1f}%)")
    
    if metodo_usado == "PLAYWRIGHT_LEAN":
        print(f"   üé≠ Playwright usado - ideal para sites com JavaScript")
    else:
        print(f"   ‚ö° Requests usado - ideal para sites est√°ticos")

# ========================
# üîÑ WRAPPER S√çNCRONO
# ========================

def main_sync():
    """üîÑ Wrapper s√≠ncrono"""
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    try:
        asyncio.run(main_pipeline())
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Opera√ß√£o cancelada pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro cr√≠tico: {e}")
        import traceback
        traceback.print_exc()
        
        # Modo emerg√™ncia
        print(f"\nüö® Modo emerg√™ncia...")
        try:
            urls_emergencia = crawler_requests(URL_BASE, min(MAX_URLS, 100), 2)
            if urls_emergencia:
                df_emergencia = pd.DataFrame(urls_emergencia)
                arquivo_emergencia = gerar_nome_arquivo_seguro(URL_BASE).replace('pipeline_', 'emergencia_')
                arquivo_emergencia = os.path.join(os.getcwd(), arquivo_emergencia)
                df_emergencia.to_excel(arquivo_emergencia, index=False)
                print(f"‚úÖ Relat√≥rio de emerg√™ncia: {arquivo_emergencia}")
        except Exception as e2:
            print(f"üí• Erro total: {e2}")

# ========================
# üöÄ ENTRY POINT
# ========================

if __name__ == "__main__":
    main_sync()