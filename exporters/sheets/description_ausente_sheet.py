# exporters/sheets/description_ausente_sheet.py - CIRÃšRGICO v3.1 RESILIENTE
# ğŸ¯ ENGINE CIRÃšRGICA: Meta description ausente/vazia + RETRY RESILIENTE
# ğŸ§  v3.1: Elimina erros transitÃ³rios de rede com backoff exponencial

import pandas as pd
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
from exporters.base_exporter import BaseSheetExporter
import time
import random

class DescriptionAusenteSheet(BaseSheetExporter):
    def __init__(self, df, writer):
        super().__init__(df, writer)
        self.session = self._criar_sessao_otimizada()
        
    def _criar_sessao_otimizada(self) -> requests.Session:
        """ğŸš€ SessÃ£o otimizada para validaÃ§Ã£o cirÃºrgica"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
        return session

    def _verificar_description_cirurgico(self, url: str, max_retries=3) -> dict:
        """ğŸ¯ VerificaÃ§Ã£o CIRÃšRGICA com RETRY RESILIENTE v3.1"""
        
        for tentativa in range(1, max_retries + 1):
            try:
                response = self.session.get(url, timeout=10, verify=False)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ğŸ” BUSCA META DESCRIPTION
                description_tag = soup.find('meta', attrs={'name': 'description'})
                
                if description_tag is None:
                    # TAG AUSENTE
                    return {
                        'url': url,
                        'sucesso': True,
                        'tem_problema': True,
                        'tipo_problema': 'TAG_AUSENTE',
                        'description_html': '[TAG NÃƒO ENCONTRADA]',
                        'description_texto': '',
                        'gravidade': 'CRITICO'
                    }
                
                # ğŸ” EXTRAI CONTENT DA TAG
                description_content = description_tag.get('content', '').strip()
                
                if not description_content:
                    # TAG VAZIA
                    return {
                        'url': url,
                        'sucesso': True,
                        'tem_problema': True,
                        'tipo_problema': 'TAG_VAZIA',
                        'description_html': str(description_tag)[:200],
                        'description_texto': '[VAZIO]',
                        'gravidade': 'CRITICO'
                    }
                
                # TAG OK - TEM CONTEÃšDO
                return {
                    'url': url,
                    'sucesso': True,
                    'tem_problema': False,
                    'tipo_problema': 'OK',
                    'description_html': str(description_tag)[:200],
                    'description_texto': description_content[:100],
                    'gravidade': 'OK'
                }
                
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, requests.exceptions.RequestException) as e:
                if tentativa == max_retries:
                    # Ãšltima tentativa falhou - retorna erro
                    return {
                        'url': url,
                        'sucesso': False,
                        'erro': str(e),
                        'tem_problema': True,
                        'tipo_problema': 'ERRO_ACESSO',
                        'description_html': '[ERRO DE ACESSO]',
                        'description_texto': '',
                        'gravidade': 'ERRO'
                    }
                else:
                    # Backoff exponencial com jitter
                    wait_time = (2 ** tentativa) + random.uniform(0, 1)
                    print(f"   âš¡ Tentativa {tentativa}/{max_retries} falhou para {url}. Retry em {wait_time:.1f}s...")
                    time.sleep(wait_time)
                    
            except Exception as e:
                if tentativa == max_retries:
                    # Ãšltima tentativa - retorna erro tÃ©cnico
                    return {
                        'url': url,
                        'sucesso': False,
                        'erro': str(e),
                        'tem_problema': True,
                        'tipo_problema': 'ERRO_ACESSO',
                        'description_html': '[ERRO DE ACESSO]',
                        'description_texto': '',
                        'gravidade': 'ERRO'
                    }
                else:
                    # Retry para outros erros tambÃ©m
                    wait_time = (2 ** tentativa) + random.uniform(0, 1)
                    print(f"   âš¡ Erro tÃ©cnico tentativa {tentativa}/{max_retries} para {url}. Retry em {wait_time:.1f}s...")
                    time.sleep(wait_time)
        
        # Fallback (nÃ£o deveria chegar aqui)
        return {
            'url': url,
            'sucesso': False,
            'erro': 'Erro inesperado no retry loop',
            'tem_problema': True,
            'tipo_problema': 'ERRO_ACESSO',
            'description_html': '[ERRO INESPERADO]',
            'description_texto': '',
            'gravidade': 'ERRO'
        }

    def _filtrar_urls_validas(self, urls_input) -> list:
        """ğŸ§¹ Remove URLs invÃ¡lidas + FILTRA STATUS 200 - VERSÃƒO UNIVERSAL"""
        
        # ğŸ”§ EXTRAÃ‡ÃƒO UNIVERSAL DE URLs
        urls_candidatas = []
        
        if isinstance(urls_input, pd.DataFrame):
            urls_candidatas = urls_input
        elif isinstance(urls_input, (list, pd.Series)):
            # Converte lista/series para DataFrame mock
            df_mock = pd.DataFrame({'url': urls_input})
            urls_candidatas = df_mock
        else:
            try:
                df_mock = pd.DataFrame({'url': list(urls_input)})
                urls_candidatas = df_mock
            except:
                print(f"âš ï¸ Erro: NÃ£o conseguiu processar input tipo {type(urls_input)}")
                return []
        
        urls_validas = []
        
        for _, row in urls_candidatas.iterrows():
            url = row.get('url', '')
            
            if not url or pd.isna(url):
                continue
            
            url_str = str(url).strip()
            
            # Filtros bÃ¡sicos
            if not url_str.startswith(('http://', 'https://')):
                continue
            
            # Remove extensÃµes nÃ£o-HTML
            if any(url_str.lower().endswith(ext) for ext in [
                '.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.rar',
                '.js', '.css', '.mp3', '.mp4', '.xml', '.json'
            ]):
                continue
            
            # ğŸ¯ FILTRO CRUCIAL: SÃ“ PÃGINAS 200 OK (se disponÃ­vel)
            status_code = row.get('status_code_http', row.get('status_code', None))
            if status_code is not None:
                try:
                    status_num = int(float(status_code))
                    if status_num != 200:
                        continue  # Pula pÃ¡ginas que nÃ£o sÃ£o 200 OK
                except (ValueError, TypeError):
                    continue  # Pula se nÃ£o conseguir converter status
            
            urls_validas.append(url_str)
        
        return list(set(urls_validas))  # Remove duplicatas

    def _verificar_descriptions_paralelo(self, urls: list) -> list:
        """ğŸš€ VerificaÃ§Ã£o paralela cirÃºrgica RESILIENTE"""
        
        print(f"ğŸ“ VerificaÃ§Ã£o RESILIENTE de descriptions iniciada: {len(urls)} URLs")
        
        resultados = []
        
        with ThreadPoolExecutor(max_workers=10) as executor:  # Reduzido para 10 por causa dos retries
            # Submete todas as URLs
            future_to_url = {
                executor.submit(self._verificar_description_cirurgico, url): url 
                for url in urls
            }
            
            # Processa resultados conforme completam
            for future in as_completed(future_to_url):
                resultado = future.result()
                resultados.append(resultado)
                
                # Progress indicator a cada 25 URLs (menos frequente por causa dos retries)
                if len(resultados) % 25 == 0:
                    print(f"âš¡ Verificados: {len(resultados)}/{len(urls)}")
        
        return resultados

    def export(self):
        """ğŸ“ Gera aba CIRÃšRGICA RESILIENTE de descriptions ausentes"""
        try:
            print(f"ğŸ“ DESCRIPTION AUSENTE - ENGINE CIRÃšRGICA v3.1 RESILIENTE")
            
            # ğŸ“‹ PREPARAÃ‡ÃƒO DOS DADOS
            urls_filtradas = self._filtrar_urls_validas(self.df)
            
            print(f"   ğŸ“Š URLs no DataFrame: {len(self.df) if hasattr(self.df, '__len__') else 'N/A'}")
            print(f"   ğŸ§¹ URLs vÃ¡lidas (200 OK): {len(urls_filtradas)}")
            print(f"   ğŸ¯ CritÃ©rio CIRÃšRGICO: Meta description ausente OU vazia")
            print(f"   ğŸ§  v3.1: Retry resiliente (3 tentativas + backoff exponencial)")
            print(f"   ğŸ” FILTRO: Apenas pÃ¡ginas com status 200 OK")
            
            if not urls_filtradas:
                print(f"   âš ï¸ Nenhuma URL vÃ¡lida para verificaÃ§Ã£o")
                self._criar_aba_vazia()
                return pd.DataFrame()
            
            # ğŸ“ VERIFICAÃ‡ÃƒO CIRÃšRGICA RESILIENTE
            resultados = self._verificar_descriptions_paralelo(urls_filtradas)
            
            # ğŸ“‹ GERA LINHAS APENAS PARA PROBLEMAS REAIS (exceto ERRO_ACESSO apÃ³s retry)
            rows = []
            erros_tecnicos = 0
            
            for resultado in resultados:
                # SÃ“ INCLUI SE TEM PROBLEMA REAL DE SEO (nÃ£o erros tÃ©cnicos)
                if resultado.get('tem_problema', False):
                    if resultado.get('tipo_problema') in ['TAG_AUSENTE', 'TAG_VAZIA']:
                        # Problema real de SEO
                        rows.append({
                            'URL': resultado['url'],
                            'Tipo_Problema': resultado['tipo_problema'],
                            'Description_HTML': resultado['description_html'],
                            'Description_Texto': resultado['description_texto'],
                            'Gravidade': resultado['gravidade']
                        })
                    elif resultado.get('tipo_problema') == 'ERRO_ACESSO':
                        # Erro tÃ©cnico apÃ³s retry - sÃ³ conta mas nÃ£o inclui
                        erros_tecnicos += 1
            
            # Se nÃ£o encontrou problemas de SEO
            if not rows:
                print(f"   ğŸ‰ PERFEITO: Todas as pÃ¡ginas tÃªm meta description com conteÃºdo!")
                if erros_tecnicos > 0:
                    print(f"   âš ï¸ {erros_tecnicos} URLs com erro tÃ©cnico (retry esgotado)")
                self._criar_aba_vazia()
                return pd.DataFrame()
            
            df_problemas = pd.DataFrame(rows)
            
            # ğŸ”„ ORDENAÃ‡ÃƒO POR GRAVIDADE
            gravidade_order = {'CRITICO': 1, 'ALTO': 2, 'MEDIO': 3, 'BAIXO': 4}
            df_problemas['sort_gravidade'] = df_problemas['Gravidade'].map(gravidade_order).fillna(99)
            df_problemas = df_problemas.sort_values(['sort_gravidade', 'URL'])
            df_problemas = df_problemas.drop('sort_gravidade', axis=1)
            
            # ğŸ“¤ EXPORTA
            df_problemas.to_excel(self.writer, index=False, sheet_name="Description_Ausente")
            
            # ğŸ“Š ESTATÃSTICAS CIRÃšRGICAS RESILIENTES
            urls_verificadas = len([r for r in resultados if r.get('sucesso', False)])
            urls_com_problemas_seo = len(rows)
            urls_perfeitas = urls_verificadas - urls_com_problemas_seo
            
            # Stats por tipo
            tag_ausente = len([r for r in rows if r['Tipo_Problema'] == 'TAG_AUSENTE'])
            tag_vazia = len([r for r in rows if r['Tipo_Problema'] == 'TAG_VAZIA'])
            
            print(f"   âœ… URLs verificadas com sucesso: {urls_verificadas}")
            print(f"   ğŸ“ URLs com problemas de SEO: {urls_com_problemas_seo}")
            print(f"   âœ¨ URLs perfeitas (description OK): {urls_perfeitas}")
            print(f"      ğŸš« Meta description ausente: {tag_ausente}")
            print(f"      ğŸ•³ï¸ Meta description vazia: {tag_vazia}")
            if erros_tecnicos > 0:
                print(f"   âš ï¸ URLs com erro tÃ©cnico (retry esgotado): {erros_tecnicos}")
            print(f"   ğŸ“‹ Aba 'Description_Ausente' criada com critÃ©rio CIRÃšRGICO v3.1")
            print(f"   ğŸ›¡ï¸ Zero falsos positivos - sÃ³ ausÃªncia/vazio real")
            print(f"   ğŸ§  Retry resiliente eliminou erros transitÃ³rios de rede")
            
            return df_problemas
            
        except Exception as e:
            print(f"âŒ Erro no engine cirÃºrgico description v3.1: {e}")
            import traceback
            traceback.print_exc()
            
            # Fallback
            self._criar_aba_vazia()
            return pd.DataFrame()

    def _criar_aba_vazia(self):
        """ğŸ“‹ Cria aba vazia quando nÃ£o hÃ¡ problemas"""
        df_vazio = pd.DataFrame(columns=[
            'URL', 'Tipo_Problema', 'Description_HTML', 'Description_Texto', 'Gravidade'
        ])
        df_vazio.to_excel(self.writer, index=False, sheet_name="Description_Ausente")